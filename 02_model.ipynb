{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-25T09:52:48.791940Z",
     "start_time": "2020-10-25T09:52:48.773496Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "#default_exp model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-25T09:52:49.738706Z",
     "start_time": "2020-10-25T09:52:48.922222Z"
    }
   },
   "outputs": [],
   "source": [
    "#exporti\n",
    "from fastai.tabular.all import * \n",
    "from tabnet.core import Sparsemax, GBN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-25T09:52:49.743219Z",
     "start_time": "2020-10-25T09:52:49.740320Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# TabNet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-25T11:28:18.286251Z",
     "start_time": "2020-10-25T11:28:18.280231Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#exporti\n",
    "def _initial_block(n_in, n_out):\n",
    "    return nn.Linear(n_in, 2*n_out, bias=False)\n",
    "\n",
    "def _rest_block(n):\n",
    "    return nn.Linear(n, 2*n, bias=False)\n",
    "\n",
    "def _create_shared_blocks(n_in, n_out, n_shared):\n",
    "    return [_initial_block(n_in, n_out)] + \\\n",
    "            [_rest_block(n_out) for _ in range(n_shared-1)]\n",
    "\n",
    "def _combine_cat_cont(x_cat, x_cont, embeds):\n",
    "    if x_cat.shape[1] != 0: \n",
    "        x = [e(x_cat[:,i]) for i,e in enumerate(embeds)]\n",
    "        x_cat = torch.cat(x, 1)\n",
    "\n",
    "    x = torch.cat([x_cat, x_cont], 1)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T17:51:16.519279Z",
     "start_time": "2020-10-24T17:51:16.515129Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class TabNetBase(Module):\n",
    "    def __init__(self, n_d=64, n_a=64, n_steps=3, n_shared_ft_blocks=2, n_independent_ft_blocks=2, \n",
    "                 virtual_batch_size=128, momentum=0.2, **kwargs):\n",
    "        store_attr()\n",
    "    \n",
    "    \n",
    "    def _create_feature_transform(self, shared_ft_blocks):\n",
    "        return FeatureTransformer(self.n_d+self.n_a, shared_ft_blocks,\n",
    "                                          self.n_independent_ft_blocks,\n",
    "                                          self.virtual_batch_size, self.momentum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T15:12:32.435254Z",
     "start_time": "2020-10-24T15:12:32.425479Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#exporti\n",
    "class TabNetEnc(TabNetBase):\n",
    "    \n",
    "    @delegates(TabNetBase.__init__)\n",
    "    def __init__(self, emb_szs, n_cont, gamma=1.5, **kwargs):        \n",
    "        store_attr()\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.embeds = nn.ModuleList([Embedding(ni, nf) for ni,nf in self.emb_szs])\n",
    "        self.n_emb = sum(e.embedding_dim for e in self.embeds)\n",
    "        self.n_features = self.n_emb + self.n_cont\n",
    "        \n",
    "        shared_ft_blocks = _create_shared_blocks(self.n_features, self.n_d + self.n_a, self.n_shared_ft_blocks)\n",
    "        \n",
    "        self.initial_ft = self._create_feature_transform(shared_ft_blocks)        \n",
    "        self.initial_bn = BatchNorm(self.n_features, ndim=1)    \n",
    "        \n",
    "        self.att_steps = nn.ModuleList([AttentiveTransformer(self.n_a, self.n_features,self.virtual_batch_size, \n",
    "                                                             self.momentum) for i in range(self.n_steps)])\n",
    "        \n",
    "        self.ft_steps = nn.ModuleList([self._create_feature_transform(shared_ft_blocks) for _ in range(self.n_steps)])        \n",
    "    \n",
    "    \n",
    "    def _split(self, x):\n",
    "        return x[:, :self.n_d], x[:, self.n_d:]\n",
    "    \n",
    "    def forward(self, x_cat, x_cont):\n",
    "        x = _combine_cat_cont(x_cat, x_cont, self.embeds)\n",
    "\n",
    "        output = []\n",
    "        x = self.initial_bn(x)\n",
    "        res = self.initial_ft(x)\n",
    "        d, a = self._split(res)\n",
    "        \n",
    "        prior = torch.ones(self.n_features, device=x_cont.device)\n",
    "        \n",
    "        for i in range(self.n_steps):\n",
    "            M = self.att_steps[i](prior, a)\n",
    "            prior = (self.gamma - M)*prior\n",
    "            masked_x = M * x\n",
    "            transformed_x = self.ft_steps[i](masked_x)\n",
    "            d, a = self._split(transformed_x)\n",
    "            output.append(nn.functional.relu(d))\n",
    "        \n",
    "        return torch.stack(output, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T15:12:32.575983Z",
     "start_time": "2020-10-24T15:12:32.569386Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#exporti\n",
    "class FeatureTransformer(Module):\n",
    "    def __init__(self, n_out, shared_layers, n_indep,\n",
    "                 virtual_batch_size=128, momentum=0.02):\n",
    "        store_attr()\n",
    "        \n",
    "        shared_layers = [FeatureTransformerBlock(layer, n_out) for layer in shared_layers]\n",
    "        independent_layers = [FeatureTransformerBlock(_rest_block(n_out), n_out) for _ in range(n_indep)]\n",
    "        \n",
    "        self.layers = nn.ModuleList([*shared_layers, *independent_layers])\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        scale = torch.sqrt(torch.FloatTensor([0.5]).to(x.device))\n",
    "        x = self.layers[0](x)\n",
    "        \n",
    "        for layer in self.layers[1:]:\n",
    "            x = scale * (x+layer(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "class FeatureTransformerBlock(nn.Sequential):\n",
    "    def __init__(self, fc, n_out,\n",
    "                 virtual_batch_size=128, momentum=0.02):\n",
    "        self.n_out = n_out\n",
    "        \n",
    "        layers = [\n",
    "                    fc, \n",
    "                    GBN(2*n_out, virtual_batch_size=virtual_batch_size, momentum=momentum),\n",
    "                    nn.GLU()\n",
    "                 ]\n",
    "        super().__init__(*layers)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T15:12:32.702226Z",
     "start_time": "2020-10-24T15:12:32.697836Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#exporti\n",
    "class AttentiveTransformer(Module):\n",
    "    \n",
    "    def __init__(self, n_a, n_in, virtual_batch_size, momentum):\n",
    "        store_attr()\n",
    "        self.fc = nn.Linear(n_a, n_in)\n",
    "        self.bn = GBN(n_in, virtual_batch_size, momentum)\n",
    "        self.sparsemax = Sparsemax()\n",
    "        \n",
    "    def forward(self, prior, a):\n",
    "        a = self.fc(a)\n",
    "        a = self.bn(a)\n",
    "        a = prior * a\n",
    "        M = self.sparsemax(a)\n",
    "        return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T15:12:33.254559Z",
     "start_time": "2020-10-24T15:12:32.849251Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N = 3\n",
    "n_cat = 5\n",
    "n_cont = 10\n",
    "n_d = n_a = 7 \n",
    "n_steps = 4\n",
    "n_out = 10\n",
    "virtual_batch_size = 5\n",
    "\n",
    "x_cont = torch.randn((N, 10))\n",
    "x_cat = torch.randint(high=3, size=(N, n_cat))\n",
    "enc = TabNetEnc([(3, 10)]*n_cat, n_cont, n_d=n_d, n_a=n_a, n_steps=n_steps)\n",
    "test_eq(enc(x_cat, x_cont).shape, (N, n_steps, n_d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Classifier Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T15:12:33.261196Z",
     "start_time": "2020-10-24T15:12:33.256324Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(TabNetBase.__init__)\n",
    "def TabNet(head_func, emb_szs, n_cont, n_out, **kwargs):    \n",
    "    class TabNetWithHead(Module): \n",
    "        def __init__(self, enc, head): store_attr()\n",
    "        def forward(self, x_cat, x_cont): return self.head(self.enc(x_cat, x_cont)) \n",
    "\n",
    "    return TabNetWithHead(TabNetEnc(emb_szs, n_cont, **kwargs), head_func(n_out, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T15:12:33.285606Z",
     "start_time": "2020-10-24T15:12:33.282750Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export \n",
    "def linear_head(n_out, n_d, **kwargs):\n",
    "    return nn.Sequential(Lambda(lambda x: x.sum(dim=1)), nn.Linear(n_d, n_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T15:12:33.436541Z",
     "start_time": "2020-10-24T15:12:33.415153Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "classifier = TabNet(head_func=linear_head, emb_szs=[(3, 10)]*n_cat, n_cont=n_cont, n_out=n_out, \n",
    "                          n_steps=n_steps, n_d=n_d, n_a=n_a, virtual_batch_size=virtual_batch_size)\n",
    "test_eq(classifier(x_cat, x_cont).shape, (N, n_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Self Supervision "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Decoder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For self-supervision, we need to create a decoder. \n",
    "The decoder receives the `x`s in a (instance_index, step_index, step_result) fashion. \n",
    "Need to chunk it to get (i) batches which correspond to the (i-th) step's output. i.e ((step_index, instance_index, step_result))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T15:12:34.470813Z",
     "start_time": "2020-10-24T15:12:34.464549Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#exporti\n",
    "class TabNetDec(TabNetBase):\n",
    "    \n",
    "    @delegates(TabNetBase.__init__)\n",
    "    def __init__(self, n_out, **kwargs):\n",
    "        store_attr()\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        shared_ft_blocks = _create_shared_blocks(self.n_d, self.n_d + self.n_a, self.n_shared_ft_blocks)\n",
    "        \n",
    "        self.steps = nn.ModuleList([\n",
    "                            nn.Sequential(\n",
    "                                self._create_feature_transform(shared_ft_blocks),\n",
    "                                nn.Linear(self.n_d+self.n_a, self.n_out)) for _ in range(self.n_steps)\n",
    "                        ])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        xs = x.chunk(self.n_steps, dim=1)\n",
    "        xs = [x.squeeze() for x in xs] #squeeze to remove the extra \"chunk\" dimension\n",
    "        \n",
    "        output = 0 \n",
    "        \n",
    "        for x,step in zip(xs, self.steps): \n",
    "            output = output + step(x)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T15:12:52.375029Z",
     "start_time": "2020-10-24T15:12:52.349467Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dec = TabNetDec(n_cont+n_cat, n_steps=n_steps, n_d=n_d, n_a=n_a, virtual_batch_size=virtual_batch_size)\n",
    "test_eq(dec(enc(x_cat, x_cont)).shape, (N, n_cont+n_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Encoder + Decoder Head = Self Supervised Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T15:12:53.501002Z",
     "start_time": "2020-10-24T15:12:53.497657Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def tabnet_decoder(n_out, **kwargs): return TabNetDec(n_out, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T15:12:59.627910Z",
     "start_time": "2020-10-24T15:12:59.596008Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tbss = TabNet(head_func=tabnet_decoder, emb_szs=[(3, 10)]*n_cat, n_cont=n_cont, n_out=n_cat+n_cont, \n",
    "                          n_steps=n_steps, n_d=n_d, n_a=n_a, virtual_batch_size=virtual_batch_size)\n",
    "test_eq(tbss(x_cat, x_cont).shape, (N, n_cat+n_cont))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Mask Generator Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-25T11:36:17.072141Z",
     "start_time": "2020-10-25T11:36:17.068280Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def create_mask(size, n_cols):\n",
    "    if n_cols is 0: return torch.ones(size)\n",
    "    rand_mat = torch.rand(*size)    \n",
    "    k_th_quant = torch.topk(rand_mat, n_cols, largest = True)[0][:,None, -1]  \n",
    "    M = rand_mat < k_th_quant\n",
    "    return M.int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-25T11:36:17.588594Z",
     "start_time": "2020-10-25T11:36:17.581939Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class TabularMasking(Callback):\n",
    "    \n",
    "    def __init__(self, n_steps, p=0.5): \n",
    "        store_attr()\n",
    "    \n",
    "    def before_batch(self):\n",
    "        x_cat, x_cont = self.xb\n",
    "        xb = torch.cat([x_cat, x_cont], dim=1)\n",
    "        n_cols = xb.shape[1]\n",
    "        n_masked = (torch.linspace(0, self.p, steps=self.n_epoch)*n_cols).floor()[self.epoch].int().item()\n",
    "        M = to_device(create_mask(xb.shape, n_masked), x_cat.device)\n",
    "        xb = xb * M\n",
    "        self.learn.xb = (xb[:, :x_cat.shape[1]].long(), xb[:, x_cat.shape[1]:])\n",
    "        self.learn.loss_func.M = M \n",
    "        self.learn.loss_func.n_steps = self.n_steps\n",
    "        \n",
    "    def after_pred(self):\n",
    "        y_cat, y_cont = self.yb\n",
    "        self.learn.yb = tuplify(torch.cat([y_cat, y_cont], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-25T11:36:18.229519Z",
     "start_time": "2020-10-25T11:36:18.224333Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tm = TabularMasking(5, 0.5)\n",
    "learn = namedtuple('a', '')\n",
    "learn.xb = torch.ones((7, 2)), torch.ones((7, 2))\n",
    "learn.n_epoch = 5\n",
    "learn.epoch = 2\n",
    "learn.loss_func = lambda x: x\n",
    "tm.learn = learn \n",
    "tm.before_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Self Supervised Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Sometimes we get a column of features that are all equal. When that happens, the `norm` becomes 0 -> divide by 0 -> `inf` loss. \n",
    "We'll fix that by removing very small values (close to 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-25T11:53:44.440047Z",
     "start_time": "2020-10-25T11:53:44.434398Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class MaskReconstructionLoss(Module):\n",
    "    def __init__(self, lambda_reg=1e-4, eps=1e-5): store_attr()\n",
    "    \n",
    "    def forward(self, preds, targ):\n",
    "        M, eps, n_steps = self.M, self.eps, self.n_steps\n",
    "        norm = (targ - targ.mean(dim=0)).pow(2).sum(dim=0).sqrt()\n",
    "        norm_mask = norm >= 1e-6\n",
    "        norm = norm[norm_mask]\n",
    "        error = (preds - targ) * M\n",
    "        error = error[:,norm_mask]\n",
    "        \n",
    "        loss = (error / norm).abs().sum()\n",
    "#         base = (M+eps).log() * (-M)\n",
    "#         res = base.sum() / (n_steps * len(M))\n",
    "#         loss += self.lambda_reg * res\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-25T11:53:45.729552Z",
     "start_time": "2020-10-25T11:53:45.720705Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1379, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tensor([[1,2],[3,4],[5,6]], dtype=float).requires_grad_()\n",
    "b = tensor([[1,2],[3,3.8],[5.2,6]], dtype=float).requires_grad_()\n",
    "\n",
    "loss_func = MaskReconstructionLoss()\n",
    "loss_func.M = torch.ones_like(a)\n",
    "loss_func.n_steps = 5\n",
    "loss = loss_func(a,b)\n",
    "loss.backward()\n",
    "test_eq(loss < 100, True)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-25T11:53:46.214644Z",
     "start_time": "2020-10-25T11:53:46.206576Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0706, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tensor([[1,2],[3,4],[5,6]], dtype=float).requires_grad_()\n",
    "b = tensor([[1,2],[1,3.8],[1,6]], dtype=float).requires_grad_()\n",
    "\n",
    "loss_func = MaskReconstructionLoss()\n",
    "loss_func.M = torch.ones_like(a)\n",
    "loss_func.n_steps = 5\n",
    "loss = loss_func(a,b)\n",
    "loss.backward()\n",
    "test_eq(loss < 100, True)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# asdf asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T15:14:37.693579Z",
     "start_time": "2020-10-24T15:14:37.690857Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from tabnet.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T15:14:38.075415Z",
     "start_time": "2020-10-24T15:14:37.937821Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "adult_path = untar_data(URLs.ADULT_SAMPLE)\n",
    "df = pd.read_csv(adult_path/'adult.csv')\n",
    "splits = RandomSplitter(valid_pct=0.2)(range_of(df))\n",
    "tabnet_args = dict(n_d=16, n_a=16, n_steps=5, virtual_batch_size=256, gamma=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T15:14:38.388145Z",
     "start_time": "2020-10-24T15:14:38.284395Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "to = TabularPandasIdentity(df, procs=[Categorify, FillMissing,Normalize],\n",
    "                   cat_names = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race'],\n",
    "                   cont_names = ['age', 'fnlwgt', 'education-num'],\n",
    "                   y_names='salary',\n",
    "                   splits=splits)\n",
    "dls = to.dataloaders(bs=1024)\n",
    "dls.n_inp = 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T15:14:38.512363Z",
     "start_time": "2020-10-24T15:14:38.489214Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = TabNetSelfSupervised(tabnet_decoder, to, **tabnet_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T15:14:38.703460Z",
     "start_time": "2020-10-24T15:14:38.699762Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn = Learner(dls, model, cbs=[TabularMasking()], loss_func=MaskReconstructionLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T15:14:49.794893Z",
     "start_time": "2020-10-24T15:14:39.344948Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-25T11:53:50.349076Z",
     "start_time": "2020-10-25T11:53:50.233262Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_core.ipynb.\n",
      "Converted 02_model.ipynb.\n",
      "No export destination, ignored:\n",
      "#exporti\n",
      "from fastai.tabular.all import * \n",
      "from tabnet.utils import *\n",
      "from tabnet.model import *\n",
      "Warning: Exporting to \"None.py\" but this module is not part of this build\n",
      "Converted 03_experiments.ipynb.\n",
      "Converted 04_self_supervision.ipynb.\n",
      "Converted 04_utils.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "304.475px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
