{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T18:00:45.475738Z",
     "start_time": "2020-10-21T18:00:45.448643Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "#default_exp model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T18:00:48.619617Z",
     "start_time": "2020-10-21T18:00:45.846721Z"
    }
   },
   "outputs": [],
   "source": [
    "#exporti\n",
    "from fastai.tabular.all import * \n",
    "from tabnet.core import Sparsemax, GBN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T18:00:48.628927Z",
     "start_time": "2020-10-21T18:00:48.622946Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TabNet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T18:00:49.730037Z",
     "start_time": "2020-10-21T18:00:49.714245Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#exporti\n",
    "def _initial_block(n_in, n_out):\n",
    "    return nn.Linear(n_in, 2*n_out, bias=False)\n",
    "\n",
    "def _rest_block(n):\n",
    "    return nn.Linear(n, 2*n, bias=False)\n",
    "\n",
    "def _create_shared_blocks(n_in, n_out, n_shared):\n",
    "    return [_initial_block(n_in, n_out)] + \\\n",
    "            [_rest_block(n_out) for _ in range(n_shared-1)]\n",
    "\n",
    "def _combine_cat_cont(x_cat, x_cont, embeds):\n",
    "    if len(x_cat) != 0: \n",
    "        x = [e(x_cat[:,i]) for i,e in enumerate(embeds)]\n",
    "        x = torch.cat(x, 1)\n",
    "\n",
    "    if len(x_cont) != 0: \n",
    "        x = torch.cat([x, x_cont], 1) if len(x_cat) != 0 else x_cont\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T18:00:50.210147Z",
     "start_time": "2020-10-21T18:00:50.199759Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class TabNetBase(Module):\n",
    "    def __init__(self, n_d=64, n_a=64, n_steps=3, n_shared_ft_blocks=2, n_independent_ft_blocks=2, virtual_batch_size=128, momentum=0.2):\n",
    "        store_attr()\n",
    "    \n",
    "    \n",
    "    def _create_feature_transform(self, shared_ft_blocks):\n",
    "        return FeatureTransformer(self.n_d+self.n_a, shared_ft_blocks,\n",
    "                                          self.n_independent_ft_blocks,\n",
    "                                          self.virtual_batch_size, self.momentum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T18:37:48.035306Z",
     "start_time": "2020-10-21T18:37:48.010154Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#exporti\n",
    "class TabNetEnc(TabNetBase):\n",
    "    \n",
    "    @delegates(TabNetBase.__init__)\n",
    "    def __init__(self, emb_szs, n_cont, gamma=1.5, **kwargs):        \n",
    "        store_attr()\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.embeds = nn.ModuleList([Embedding(ni, nf) for ni,nf in self.emb_szs])\n",
    "        self.n_emb = sum(e.embedding_dim for e in self.embeds)\n",
    "        self.n_features = self.n_emb + self.n_cont\n",
    "        \n",
    "        shared_ft_blocks = _create_shared_blocks(self.n_features, self.n_d + self.n_a, self.n_shared_ft_blocks)\n",
    "        \n",
    "        self.initial_ft = self._create_feature_transform(shared_ft_blocks)        \n",
    "        self.initial_bn = BatchNorm(self.n_features, ndim=1)    \n",
    "        \n",
    "        self.att_steps = nn.ModuleList([AttentiveTransformer(self.n_a, self.n_features,self.virtual_batch_size, \n",
    "                                                             self.momentum) for i in range(self.n_steps)])\n",
    "        \n",
    "        self.ft_steps = nn.ModuleList([self._create_feature_transform(shared_ft_blocks) for _ in range(self.n_steps)])        \n",
    "    \n",
    "    \n",
    "    def _split(self, x):\n",
    "        return x[:, :self.n_d], x[:, self.n_d:]\n",
    "    \n",
    "    def forward(self, x_cat, x_cont):\n",
    "        x = _combine_cat_cont(x_cat, x_cont, self.embeds)\n",
    "\n",
    "        output = []\n",
    "        x = self.initial_bn(x)\n",
    "        res = self.initial_ft(x)\n",
    "        d, a = self._split(res)\n",
    "        \n",
    "        prior = torch.ones(self.n_features, device=x_cont.device)\n",
    "        \n",
    "        for i in range(self.n_steps):\n",
    "            M = self.att_steps[i](prior, a)\n",
    "            prior = (self.gamma - M)*prior\n",
    "            masked_x = M * x\n",
    "            transformed_x = self.ft_steps[i](masked_x)\n",
    "            d, a = self._split(transformed_x)\n",
    "            output.append(nn.functional.relu(d))\n",
    "        \n",
    "        return torch.stack(output, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T18:37:56.368956Z",
     "start_time": "2020-10-21T18:37:56.354031Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#exporti\n",
    "class FeatureTransformer(Module):\n",
    "    def __init__(self, n_out, shared_layers, n_indep,\n",
    "                 virtual_batch_size=128, momentum=0.02):\n",
    "        store_attr()\n",
    "        \n",
    "        shared_layers = [FeatureTransformerBlock(layer, n_out) for layer in shared_layers]\n",
    "        independent_layers = [FeatureTransformerBlock(_rest_block(n_out), n_out) for _ in range(n_indep)]\n",
    "        \n",
    "        self.layers = nn.ModuleList([*shared_layers, *independent_layers])\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        scale = torch.sqrt(torch.FloatTensor([0.5]).to(x.device))\n",
    "        x = self.layers[0](x)\n",
    "        \n",
    "        for layer in self.layers[1:]:\n",
    "            x = scale * (x+layer(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "class FeatureTransformerBlock(nn.Sequential):\n",
    "    def __init__(self, fc, n_out,\n",
    "                 virtual_batch_size=128, momentum=0.02):\n",
    "        self.n_out = n_out\n",
    "        \n",
    "        layers = [\n",
    "                    fc, \n",
    "                    GBN(2*n_out, virtual_batch_size=virtual_batch_size, momentum=momentum),\n",
    "                    nn.GLU()\n",
    "                 ]\n",
    "        super().__init__(*layers)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T18:37:56.531906Z",
     "start_time": "2020-10-21T18:37:56.520577Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#exporti\n",
    "class AttentiveTransformer(Module):\n",
    "    \n",
    "    def __init__(self, n_a, n_in, virtual_batch_size, momentum):\n",
    "        store_attr()\n",
    "        self.fc = nn.Linear(n_a, n_in)\n",
    "        self.bn = GBN(n_in, virtual_batch_size, momentum)\n",
    "        self.sparsemax = Sparsemax()\n",
    "        \n",
    "    def forward(self, prior, a):\n",
    "        a = self.fc(a)\n",
    "        a = self.bn(a)\n",
    "        a = prior * a\n",
    "        M = self.sparsemax(a)\n",
    "        return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T18:21:00.162205Z",
     "start_time": "2020-10-21T18:21:00.097924Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N = 3\n",
    "n_cat = 5\n",
    "n_cont = 10\n",
    "n_d = n_a = 7 \n",
    "n_steps = 4\n",
    "n_out = 10\n",
    "virtual_batch_size = 5\n",
    "\n",
    "x_cont = torch.randn((N, 10))\n",
    "x_cat = torch.randint(high=3, size=(N, n_cat))\n",
    "enc = TabNetEnc([(3, 10)]*n_cat, n_cont, n_d=n_d, n_a=n_a, n_steps=n_steps)\n",
    "test_eq(enc(x_cat, x_cont).shape, (N, n_steps, n_d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T19:00:35.611160Z",
     "start_time": "2020-10-21T19:00:35.601326Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(TabNetBase.__init__)\n",
    "def TabNet(head_func, emb_szs, n_cont, n_out, **kwargs):    \n",
    "    class TabNetWithHead(Module): \n",
    "        def __init__(self, enc, head): store_attr()\n",
    "        def forward(self, x_cat, x_cont): return self.head(self.enc(x_cat, x_cont)) \n",
    "\n",
    "    return TabNetWithHead(TabNetEnc(emb_szs, n_cont, **kwargs), head_func(n_out, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T19:00:35.790813Z",
     "start_time": "2020-10-21T19:00:35.777665Z"
    }
   },
   "outputs": [],
   "source": [
    "#export \n",
    "def linear_head(n_out, n_d, **kwargs):\n",
    "    return nn.Sequential(Lambda(lambda x: x.sum(dim=1)), nn.Linear(n_d, n_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T19:00:36.122749Z",
     "start_time": "2020-10-21T19:00:35.963425Z"
    }
   },
   "outputs": [],
   "source": [
    "classifier = TabNet(head_func=linear_head, emb_szs=[(3, 10)]*n_cat, n_cont=n_cont, n_out=n_out, \n",
    "                          n_steps=n_steps, n_d=n_d, n_a=n_a, virtual_batch_size=virtual_batch_size)\n",
    "test_eq(classifier(x_cat, x_cont).shape, (N, n_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Self Supervision "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Decoder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For self-supervision, we need to create a decoder. \n",
    "The decoder receives the `x`s in a (instance_index, step_index, step_result) fashion. \n",
    "Need to chunk it to get (i) batches which correspond to the (i-th) step's output. i.e ((step_index, instance_index, step_result))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T17:56:02.682123Z",
     "start_time": "2020-10-21T17:56:02.651616Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#exporti\n",
    "class TabNetDec(TabNetBase):\n",
    "    \n",
    "    @delegates(TabNetBase.__init__)\n",
    "    def __init__(self, n_out, **kwargs):\n",
    "        store_attr()\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        shared_ft_blocks = _create_shared_blocks(self.n_d, self.n_d + self.n_a, self.n_shared_ft_blocks)\n",
    "        \n",
    "        self.steps = nn.ModuleList([\n",
    "                            nn.Sequential(\n",
    "                                self._create_feature_transform(shared_ft_blocks),\n",
    "                                nn.Linear(self.n_d+self.n_a, self.n_out)) for _ in range(self.n_steps)\n",
    "                        ])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        xs = x.chunk(self.n_steps, dim=1)\n",
    "        xs = [x.squeeze() for x in xs] #squeeze to remove the extra \"chunk\" dimension\n",
    "        \n",
    "        output = 0 \n",
    "        \n",
    "        for x,step in zip(xs, self.steps): \n",
    "            output = output + step(x)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T17:56:02.827343Z",
     "start_time": "2020-10-21T17:56:02.688319Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dec = TabNetDec(n_cont+n_cat, n_steps=n_steps, n_d=n_d, n_a=n_a, virtual_batch_size=virtual_batch_size)\n",
    "test_eq(dec(enc(x)).shape, (N, n_cont+n_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Encoder + Decoder Head = Self Supervised Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T17:56:02.847465Z",
     "start_time": "2020-10-21T17:56:02.837049Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def tabnet_decoder(n_out, **kwargs): return TabNetDec(n_out, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T17:56:03.012837Z",
     "start_time": "2020-10-21T17:56:02.857961Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tbss = TabNet(head_func=tabnet_decoder, emb_szs=[(3, 10)]*n_cat, n_cont=n_cont, n_out=n_cat+n_cont, \n",
    "                          n_steps=n_steps, n_d=n_d, n_a=n_a, virtual_batch_size=virtual_batch_size)\n",
    "test_eq(tbss(x).shape, (N, n_cat+n_cont))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Mask Generator Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T17:56:03.034968Z",
     "start_time": "2020-10-21T17:56:03.016553Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def create_mask(size, n_cols):\n",
    "    if n_cols is 0: return torch.ones(size)\n",
    "    rand_mat = torch.rand(*size)    \n",
    "    k_th_quant = torch.topk(rand_mat, n_cols, largest = True)[0][:,None, -1]  \n",
    "    M = rand_mat < k_th_quant\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T17:56:03.061853Z",
     "start_time": "2020-10-21T17:56:03.038130Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class TabularMasking(Callback):\n",
    "    \n",
    "    def __init__(self, p=0.5): \n",
    "        store_attr()\n",
    "    \n",
    "    def before_batch(self):\n",
    "        x_cat, x_cont = self.xb\n",
    "        xb = torch.cat([x_cat, x_cont], dim=1)\n",
    "        n_cols = xb.shape[1]\n",
    "        n_masked = (torch.linspace(0, self.p, steps=self.n_epoch)*n_cols).floor()[self.epoch].int().item()\n",
    "        M = create_mask(xb.shape, n_masked)\n",
    "        xb = xb * M\n",
    "        self.learn.xb = (xb[:, :x_cat.shape[1]].long(), xb[:, x_cat.shape[1]:])\n",
    "        self.learn.loss_func.M = M \n",
    "        \n",
    "    def after_pred(self):\n",
    "        y_cat, y_cont = self.yb\n",
    "        self.learn.yb = tuplify(torch.cat([y_cat, y_cont], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T17:56:03.090065Z",
     "start_time": "2020-10-21T17:56:03.067673Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tm = TabularMasking(0.5)\n",
    "learn = namedtuple('a', '')\n",
    "learn.xb = torch.ones((7, 2)), torch.ones((7, 2))\n",
    "learn.n_epoch = 5\n",
    "learn.epoch = 2\n",
    "learn.loss_func = lambda x: x\n",
    "tm.learn = learn \n",
    "tm.before_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Self Supervised Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T17:56:03.111801Z",
     "start_time": "2020-10-21T17:56:03.095807Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class MaskReconstructionLoss(Module):\n",
    "    \n",
    "    def __init__(self): pass\n",
    "    \n",
    "    def forward(self, preds, targ):\n",
    "        norm = (targ - targ.mean(dim=0)).pow(2).sum(dim=0).sqrt()\n",
    "        error_masked = (preds - targ) * self.M\n",
    "        \n",
    "        return (error_masked / norm).abs().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T17:56:03.182700Z",
     "start_time": "2020-10-21T17:56:03.120032Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = tensor([[1,2],[3,4]], dtype=float).requires_grad_()\n",
    "b = tensor([[1,2],[3,3.8]], dtype=float).requires_grad_()\n",
    "\n",
    "loss_func = MaskReconstructionLoss()\n",
    "loss_func.M = torch.ones_like(a)\n",
    "loss_func(a,b).backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T19:00:38.815323Z",
     "start_time": "2020-10-21T19:00:37.853450Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_core.ipynb.\n",
      "Converted 02_model.ipynb.\n",
      "No export destination, ignored:\n",
      "#exporti\n",
      "from fastai.tabular.all import * \n",
      "from tabnet.utils import *\n",
      "from tabnet.model import *\n",
      "Warning: Exporting to \"None.py\" but this module is not part of this build\n",
      "Converted 03_experiments.ipynb.\n",
      "Converted 04_self_supervision.ipynb.\n",
      "Converted 04_utils.ipynb.\n",
      "Converted Untitled.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "304.475px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
