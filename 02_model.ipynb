{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T07:48:33.713501Z",
     "start_time": "2020-10-30T07:48:33.694389Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "#default_exp model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T07:48:34.670116Z",
     "start_time": "2020-10-30T07:48:33.868539Z"
    }
   },
   "outputs": [],
   "source": [
    "#exporti\n",
    "from fastai.tabular.all import * \n",
    "from tabnet.core import Sparsemax, GBN\n",
    "from mock import Mock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T07:48:34.675632Z",
     "start_time": "2020-10-30T07:48:34.672152Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TabNet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T16:37:57.224125Z",
     "start_time": "2020-10-28T16:37:57.217305Z"
    }
   },
   "outputs": [],
   "source": [
    "#exporti\n",
    "def _initial_block(n_in, n_out):\n",
    "    return nn.Linear(n_in, 2*n_out, bias=False)\n",
    "\n",
    "def _rest_block(n):\n",
    "    return nn.Linear(n, 2*n, bias=False)\n",
    "\n",
    "def _create_shared_blocks(n_in, n_out, n_shared):\n",
    "    return [_initial_block(n_in, n_out)] + \\\n",
    "            [_rest_block(n_out) for _ in range(n_shared-1)]\n",
    "\n",
    "def _combine_cat_cont(x_cat, x_cont, embeds):\n",
    "    if x_cat.shape[1] != 0: \n",
    "        x = [e(x_cat[:,i]) for i,e in enumerate(embeds)]\n",
    "        x_cat = torch.cat(x, 1)\n",
    "\n",
    "    x = torch.cat([x_cat, x_cont], 1)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T16:37:57.230986Z",
     "start_time": "2020-10-28T16:37:57.226038Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class TabNetBase(Module):\n",
    "    def __init__(self, n_d=64, n_a=64, n_steps=3, n_shared_ft_blocks=2, n_independent_ft_blocks=2, \n",
    "                 virtual_batch_size=128, momentum=0.2, **kwargs):\n",
    "        store_attr()\n",
    "    \n",
    "    \n",
    "    def _create_feature_transform(self, shared_ft_blocks):\n",
    "        return FeatureTransformer(self.n_d+self.n_a, shared_ft_blocks,\n",
    "                                          self.n_independent_ft_blocks,\n",
    "                                          self.virtual_batch_size, self.momentum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Notice the prior in the `forward` method. It's set by the `SetPrior` callback later. I've put it in a callback as we can choose different types of prior methods. \n",
    "* Notice that the output of the encoder is the stacked outputs of each step (so later layers can "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T18:29:57.916287Z",
     "start_time": "2020-10-28T18:29:57.905462Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#exporti\n",
    "class TabNetEnc(TabNetBase):\n",
    "    \n",
    "    @delegates(TabNetBase.__init__)\n",
    "    def __init__(self, emb_szs, n_cont, gamma=1.5, **kwargs):        \n",
    "        store_attr()\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.embeds = nn.ModuleList([Embedding(ni, nf) for ni,nf in self.emb_szs])\n",
    "        self.n_emb = sum(e.embedding_dim for e in self.embeds)\n",
    "        self.n_features = self.n_emb + self.n_cont\n",
    "        \n",
    "        shared_ft_blocks = _create_shared_blocks(self.n_features, self.n_d + self.n_a, self.n_shared_ft_blocks)\n",
    "        \n",
    "        self.initial_ft = self._create_feature_transform(shared_ft_blocks)        \n",
    "        self.initial_bn = BatchNorm(self.n_features, ndim=1)    \n",
    "        \n",
    "        self.att_steps = nn.ModuleList([AttentiveTransformer(self.n_a, self.n_features,self.virtual_batch_size, \n",
    "                                                             self.momentum) for i in range(self.n_steps)])\n",
    "        \n",
    "        self.ft_steps = nn.ModuleList([self._create_feature_transform(shared_ft_blocks) for _ in range(self.n_steps)])        \n",
    "    \n",
    "    \n",
    "    def _split(self, x):\n",
    "        return x[:, :self.n_d], x[:, self.n_d:]\n",
    "    \n",
    "    def forward(self, x_cat, x_cont):\n",
    "        x = _combine_cat_cont(x_cat, x_cont, self.embeds)\n",
    "\n",
    "        x = x * self.prior \n",
    "        \n",
    "        output = []\n",
    "        x = self.initial_bn(x)\n",
    "        res = self.initial_ft(x)\n",
    "        d, a = self._split(res)\n",
    "        \n",
    "        self.masks = []\n",
    "        for i in range(self.n_steps):\n",
    "            M = self.att_steps[i](self.prior, a)\n",
    "            self.masks.append(M)\n",
    "            self.prior = (self.gamma - M)*self.prior\n",
    "            masked_x = M * x\n",
    "            transformed_x = self.ft_steps[i](masked_x)\n",
    "            d, a = self._split(transformed_x)\n",
    "            output.append(nn.functional.relu(d))\n",
    "        \n",
    "        return torch.stack(output, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T18:29:58.140896Z",
     "start_time": "2020-10-28T18:29:58.133611Z"
    }
   },
   "outputs": [],
   "source": [
    "#exporti\n",
    "class FeatureTransformer(Module):\n",
    "    def __init__(self, n_out, shared_layers, n_indep,\n",
    "                 virtual_batch_size=128, momentum=0.02):\n",
    "        store_attr()\n",
    "        \n",
    "        shared_layers = [FeatureTransformerBlock(layer, n_out, virtual_batch_size, momentum) \n",
    "                             for layer in shared_layers]\n",
    "        independent_layers = [FeatureTransformerBlock(_rest_block(n_out), n_out, virtual_batch_size, momentum) \n",
    "                              for _ in range(n_indep)]\n",
    "        \n",
    "        self.layers = nn.ModuleList([*shared_layers, *independent_layers])\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        scale = torch.sqrt(torch.FloatTensor([0.5]).to(x.device))\n",
    "        x = self.layers[0](x)\n",
    "        \n",
    "        for layer in self.layers[1:]:\n",
    "            x = scale * (x+layer(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "class FeatureTransformerBlock(nn.Sequential):\n",
    "    def __init__(self, fc, n_out,\n",
    "                 virtual_batch_size=128, momentum=0.02):\n",
    "        self.n_out = n_out\n",
    "        \n",
    "        layers = [\n",
    "                    fc, \n",
    "                    GBN(2*n_out, virtual_batch_size=virtual_batch_size, momentum=momentum),\n",
    "                    nn.GLU()\n",
    "                 ]\n",
    "        super().__init__(*layers)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T18:31:37.588510Z",
     "start_time": "2020-10-28T18:31:37.583795Z"
    }
   },
   "outputs": [],
   "source": [
    "#exporti\n",
    "class AttentiveTransformer(Module):\n",
    "    \n",
    "    def __init__(self, n_in, n_out, virtual_batch_size, momentum):\n",
    "        store_attr()\n",
    "        self.fc = nn.Linear(n_in, n_out)\n",
    "        self.bn = GBN(n_out, virtual_batch_size, momentum)\n",
    "        self.sparsemax = Sparsemax()\n",
    "        \n",
    "    def forward(self, prior, a):\n",
    "        a = self.fc(a)\n",
    "        a = self.bn(a)\n",
    "        a = prior * a\n",
    "        M = self.sparsemax(a)\n",
    "        return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T18:31:38.356607Z",
     "start_time": "2020-10-28T18:31:38.333719Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N = 3\n",
    "n_cat = 5\n",
    "n_cont = 10\n",
    "n_d = n_a = 7 \n",
    "n_steps = 4\n",
    "n_out = 10\n",
    "virtual_batch_size = 5\n",
    "\n",
    "x_cont = torch.randn((N, 10))\n",
    "x_cat = torch.randint(high=3, size=(N, n_cat))\n",
    "enc = TabNetEnc([(3, 10)]*n_cat, n_cont, n_d=n_d, n_a=n_a, n_steps=n_steps)\n",
    "enc.prior = 1 \n",
    "test_eq(enc(x_cat, x_cont).shape, (N, n_steps, n_d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Simple Classifier Head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Work with the regular CE loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T16:37:57.303406Z",
     "start_time": "2020-10-28T16:37:57.297856Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(TabNetBase.__init__)\n",
    "def TabNet(head_func, emb_szs, n_cont, n_out, **kwargs):    \n",
    "    class TabNetWithHead(Module): \n",
    "        def __init__(self, enc, head): store_attr()\n",
    "        def forward(self, x_cat, x_cont): return self.head(self.enc(x_cat, x_cont)) \n",
    "\n",
    "    return TabNetWithHead(TabNetEnc(emb_szs, n_cont, **kwargs), head_func(n_out, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T16:37:57.311988Z",
     "start_time": "2020-10-28T16:37:57.306684Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export \n",
    "def linear_head(n_out, n_d, **kwargs):\n",
    "    return nn.Sequential(Lambda(lambda x: x.sum(dim=1)), nn.Linear(n_d, n_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T16:37:57.343634Z",
     "start_time": "2020-10-28T16:37:57.314887Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "classifier = TabNet(head_func=linear_head, emb_szs=[(3, 10)]*n_cat, n_cont=n_cont, n_out=n_out, \n",
    "                          n_steps=n_steps, n_d=n_d, n_a=n_a, virtual_batch_size=virtual_batch_size)\n",
    "classifier.enc.prior = 1 \n",
    "test_eq(classifier(x_cat, x_cont).shape, (N, n_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Mask Regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T16:47:37.021351Z",
     "start_time": "2020-10-28T16:47:37.015608Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class MaskRegularizer(Callback):\n",
    "    def __init__(self, lambda_sparse=1e-4, eps=1e-6):\n",
    "        store_attr()\n",
    "        self.loss_regs = []\n",
    "    \n",
    "    def after_loss(self):\n",
    "        masks = self.model.enc.masks\n",
    "        Ms = torch.stack(masks)\n",
    "        n_steps,B,_ = Ms.shape\n",
    "        res = ((Ms + self.eps).log()*(-Ms)/(n_steps * B)).sum()\n",
    "        self.loss_regs.append(res)\n",
    "        self.learn.loss = self.loss + self.lambda_sparse*res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T16:47:37.723112Z",
     "start_time": "2020-10-28T16:47:37.719551Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class SetPrior(Callback):\n",
    "    def before_batch(self):\n",
    "        self.learn.model.enc.prior = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Supervision "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Decoder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For self-supervision, we need to create a decoder. \n",
    "The decoder receives the `x`s in a (instance_index, step_index, step_result) fashion. \n",
    "Need to chunk it to get (i) batches which correspond to the (i-th) step's output. i.e ((step_index, instance_index, step_result))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T16:37:57.369073Z",
     "start_time": "2020-10-28T16:37:57.359909Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#exporti\n",
    "class TabNetDec(TabNetBase):\n",
    "    \n",
    "    @delegates(TabNetBase.__init__)\n",
    "    def __init__(self, n_out, **kwargs):\n",
    "        store_attr()\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        shared_ft_blocks = _create_shared_blocks(self.n_d, self.n_d + self.n_a, self.n_shared_ft_blocks)\n",
    "        \n",
    "        self.steps = nn.ModuleList([\n",
    "                            nn.Sequential(\n",
    "                                self._create_feature_transform(shared_ft_blocks),\n",
    "                                nn.Linear(self.n_d+self.n_a, self.n_out)) for _ in range(self.n_steps)\n",
    "                        ])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        xs = x.chunk(self.n_steps, dim=1)\n",
    "        xs = [x.squeeze() for x in xs] #squeeze to remove the extra \"chunk\" dimension\n",
    "        \n",
    "        output = 0 \n",
    "        \n",
    "        for x,step in zip(xs, self.steps): \n",
    "            output = output + step(x)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T16:37:57.399051Z",
     "start_time": "2020-10-28T16:37:57.370949Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dec = TabNetDec(n_cont+n_cat, n_steps=n_steps, n_d=n_d, n_a=n_a, virtual_batch_size=virtual_batch_size)\n",
    "test_eq(dec(enc(x_cat, x_cont)).shape, (N, n_cont+n_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Encoder + Decoder Head = Self Supervised Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T16:37:57.407030Z",
     "start_time": "2020-10-28T16:37:57.401334Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def tabnet_decoder(n_out, **kwargs): return TabNetDec(n_out, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T16:37:57.442622Z",
     "start_time": "2020-10-28T16:37:57.409208Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tbss = TabNet(head_func=tabnet_decoder, emb_szs=[(3, 10)]*n_cat, n_cont=n_cont, n_out=n_cat+n_cont, \n",
    "                          n_steps=n_steps, n_d=n_d, n_a=n_a, virtual_batch_size=virtual_batch_size)\n",
    "tbss.enc.prior = 1 \n",
    "test_eq(tbss(x_cat, x_cont).shape, (N, n_cat+n_cont))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Mask Generator Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T17:42:52.417709Z",
     "start_time": "2020-10-29T17:42:52.413451Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def create_mask(size, n_cols):\n",
    "    if n_cols is 0: return torch.ones(size)\n",
    "    rand_mat = torch.rand(*size)    \n",
    "    k_th_quant = torch.topk(rand_mat, n_cols, largest = True)[0][:,None, -1]  \n",
    "    M = rand_mat < k_th_quant\n",
    "    return M.int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T17:42:52.728121Z",
     "start_time": "2020-10-29T17:42:52.721101Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class TabularMasking(Callback):\n",
    "    \n",
    "    def __init__(self, p=0.5, curriculum=True): \n",
    "        store_attr()\n",
    "    \n",
    "    def before_batch(self):\n",
    "        x_cat, x_cont = self.xb\n",
    "        xb = torch.cat([x_cat, x_cont], dim=1)\n",
    "        n_cols = xb.shape[1]\n",
    "        if self.curriculum: \n",
    "            n_masked = (torch.linspace(0, self.p, steps=self.n_epoch)*n_cols).floor()[self.epoch].int().item()\n",
    "        else:\n",
    "            n_masked = torch.distributions.Binomial(n_cols, probs=self.p).sample().int().item()\n",
    "        S = to_device(create_mask(xb.shape, n_masked), x_cat.device)\n",
    "        xb = xb * S\n",
    "        self.learn.xb = (xb[:, :x_cat.shape[1]].long(), xb[:, x_cat.shape[1]:])\n",
    "        self.learn.loss_func.S = S \n",
    "        \n",
    "    def after_pred(self):\n",
    "        y_cat, y_cont = self.yb\n",
    "        self.learn.yb = tuplify(torch.cat([y_cat, y_cont], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T17:42:53.076514Z",
     "start_time": "2020-10-29T17:42:53.056427Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tm = TabularMasking(0.5)\n",
    "learn = Mock()\n",
    "learn.xb = torch.ones((7, 2)), torch.ones((7, 2))\n",
    "learn.n_epoch = 5\n",
    "learn.epoch = 2\n",
    "learn.loss_func = Mock()\n",
    "learn.model = Mock()\n",
    "tm.learn = learn \n",
    "tm.before_batch()\n",
    "test_eq(learn.loss_func.S.shape, (7,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self Supervised Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we get a column of features that are all equal. When that happens, the `norm` becomes 0 -> divide by 0 -> `inf` loss. \n",
    "We'll fix that by removing very small values (close to 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T17:42:56.815648Z",
     "start_time": "2020-10-29T17:42:56.810294Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class MaskReconstructionLoss(Module):\n",
    "    def __init__(self, lambda_reg=1e-4, eps=1e-5): store_attr()\n",
    "    \n",
    "    def forward(self, preds, targ):\n",
    "        norm = (targ - targ.mean(dim=0)).pow(2).sum(dim=0).sqrt()\n",
    "        norm_mask = norm >= 1e-6\n",
    "        norm = norm[norm_mask]\n",
    "        error = (preds - targ) * (1-self.S)\n",
    "        error = error[:,norm_mask]\n",
    "        \n",
    "        loss = (error / norm).abs().sum()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T17:42:57.282932Z",
     "start_time": "2020-10-29T17:42:57.235257Z"
    }
   },
   "outputs": [],
   "source": [
    "a = tensor([[1,2],[3,4],[5,6]], dtype=float).requires_grad_()\n",
    "b = tensor([[1,2],[3,3.8],[5.2,6]], dtype=float).requires_grad_()\n",
    "\n",
    "loss_func = MaskReconstructionLoss()\n",
    "loss_func.S = torch.ones_like(a)\n",
    "loss = loss_func(a,b)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T17:42:57.703873Z",
     "start_time": "2020-10-29T17:42:57.697362Z"
    }
   },
   "outputs": [],
   "source": [
    "class RecreatedLoss(Module):\n",
    "    \"Measures how well we have created the original tabular inputs\"\n",
    "    def __init__(self, cat_dict):\n",
    "        ce = CrossEntropyLossFlat(reduction='sum')\n",
    "        mse = MSELossFlat(reduction='sum')\n",
    "        store_attr('cat_dict,ce,mse')\n",
    "\n",
    "    def forward(self, preds, cat_targs, cont_targs):\n",
    "        cats, conts = preds\n",
    "        tot_ce, pos = cats.new([0]), 0\n",
    "        for i, (k,v) in enumerate(self.cat_dict.items()):\n",
    "            tot_ce += self.ce(cats[:, pos:pos+v], cat_targs[:,i])\n",
    "            pos += v\n",
    "        \n",
    "        norm_cats = cats.new([len(self.cat_dict)])\n",
    "        norm_conts = conts.new([conts.size(1)])\n",
    "        cat_loss = tot_ce/norm_cats\n",
    "        cont_loss = self.mse(conts, cont_targs)/norm_conts\n",
    "        total = cat_loss+cont_loss\n",
    "\n",
    "        return total / cats.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecreateHead(Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T18:31:47.987764Z",
     "start_time": "2020-10-28T18:31:47.899278Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_core.ipynb.\n",
      "Converted 02_model.ipynb.\n",
      "Converted 03_experiments.ipynb.\n",
      "Converted 04_utils.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "304.475px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
