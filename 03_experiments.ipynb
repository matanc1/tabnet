{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T16:22:09.649432Z",
     "start_time": "2020-10-29T16:22:09.633434Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T16:22:10.611685Z",
     "start_time": "2020-10-29T16:22:09.881403Z"
    }
   },
   "outputs": [],
   "source": [
    "from fastai.tabular.all import * \n",
    "from tabnet.utils import *\n",
    "from tabnet.model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T16:22:10.616640Z",
     "start_time": "2020-10-29T16:22:10.613338Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "\n",
    "I propose a method for tabular learning problems where we have an abundance of data but a small amount of it is labeled based on the TabNet architecture.\n",
    "I demonstrated that by using semi-supervised learning we can improve the performance of the model in the small labeled set setting and check what ammount of data is enough.\n",
    "I also demonstrated that curriculum learning improves this by improving the self-supervised step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Tabular data problems are still very prevalent in today's world, especially in big corporations that amass large amounts of data for analysis.\n",
    "\n",
    "Even though this domain is popular, it's not as widely researched as computer vision, audio etc. For example, there are [papers](https://arxiv.org/abs/1604.07379) using self-supervised learning in CV problems as far back as 2016, while the first known one for Tabular data has been released in August 2019. \n",
    "\n",
    "Even though large corporations usually have large amounts of data, in many of their tabular problems they have very few labeled examples as those are very expensive to get. To address the scenario where there isn't an abundance of labeled data, the common approach is to use oversampling methods such as [SMOTE](https://arxiv.org/pdf/1106.1813.pdf). Even though these methods sometimes improve the model's performance, the improvement is usually minor at best.\n",
    "\n",
    "For these reasons, I wanted to implement a self-supervised approach for Tabular Data by learning the underlying representation and then using the pretrained model with the labeled data we have. \n",
    "\n",
    "In this project I wanted to test:  \n",
    "1. If a model trained in a self-supervised fashion gives better results in the small labeled setting.\n",
    "1. At what number of samples is self supervision unnecesary. \n",
    "1. If and how curriculum learning improves the outcome and the `self supervised` step. \n",
    "\n",
    "To do so I've implemented a relatively new (Aug 2019) Tabular Data DL model called [Tabnet](#https://arxiv.org/pdf/1908.07442.pdf) which uses sequential attention to choose which features to look at at each step, as well as introduced tabular self-supervision for the first time (although I couldn't find any implementation of the self supervision which is why I wanted to implement it). Furthermore, TabNet also enables interpretability by using the sequential attention. I've yet to implement this feature but plan on doing so. \n",
    "I've also taken the time to learn the [fastai framework](https://docs.fast.ai/) (a DL framework implemented using `pytorch`) for this project which helped me decouple the different parts and run experiments efficiently. \n",
    "\n",
    "I've tested this approach on 2 different datasets: \n",
    "1. Adult Census Income - where the task is to distinguish whether a person's income is above $50,000\n",
    "1. Forest Cover - classifying the forst cover type from cartographic variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Basis for our model - TabNet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The TabNet architecture uses an encoder-head architecture. \n",
    "The encoder is used to learn a better representation of the features in a sequential manner by using masked attention. It is the focus of the TabNet paper. \n",
    "The head (a simple FC block for example) then receives the encoder's output to solve the task at hand (classification / regression / decoding). \n",
    "\n",
    "\n",
    "##### Encoder\n",
    "![image.png](attachment:image.png)\n",
    "TabNet's encoder works by sequentially calculating masks (using an attention block) to be applied to the features. The masked features are then transformed at each step. Half of the transformed features will be used by the decoder, while the other half will be used by the next step's attention block.\n",
    "\n",
    "The Encoder is built from 2 basic blocks: \n",
    "1. Feature Transformer - multiple stacks of blocks made up of FC, BN, GLUs with residual connections. The first few blocks are usually shared since the input's transformations should be the same across all steps. \n",
    "1. Attentive Transformer - creates the mask. A block consisting of a FC, BN and Sparsemax activation (with an additional prior to make sure that the same features won't be used too many times).\n",
    "\n",
    "\n",
    "##### Head \n",
    "A simple layer that consists of adding up all the outputs from the encoder's steps and passing them through a FC layer. \n",
    "\n",
    "\n",
    "##### Self Supervised Training \n",
    "![image.png](attachment:image.png)\n",
    "The self supervised training works by creating a mask `S` and applying it to keep some of the features, and then trying to reconstruct the `1-S` left over features. \n",
    "To implement the self supervised training, we need to replace the problem's loss (MSE, CE etc) with a loss that takes the forme into account as well as change the model's head to a decoder. \n",
    "\n",
    "1. Decoder - As seen above, we used the proposed architecture of a `Feature Transformer` for each step accompanied by a FC layer and then adding up all the results. \n",
    "1. Loss - For the loss we used the proposed `Reconstruction Loss` which is similar to MSE/MAE for the reconstructed (non masked) features, as well as adding a regularization term (since they're scaled differently). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Changes I made "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Self Supervised training \n",
    "\n",
    "I've implemented the self supervised training in a Curricular Learning fashion - instead of choosing the mask a feature with some `p` probability which creates a varying number of masked features at every iteration, we progressively mask more features (make the problem harder) as the number of iterations grow.\n",
    "\n",
    "\n",
    "1. RNN\n",
    "1. Self Supervised Loss\n",
    "1. Dropout "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "I've tested this approach on 2 different datasets: \n",
    "1. Adult Census Income - where the task is to distinguish whether a person's income is above $50,000\n",
    "1. Forest Cover - classifying the forst cover type from cartographic variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Adult "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T19:35:46.483486Z",
     "start_time": "2020-10-28T19:35:46.425316Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "adult_path = untar_data(URLs.ADULT_SAMPLE)\n",
    "df = pd.read_csv(adult_path/'adult.csv')\n",
    "params = dict(cat_names = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race'],\n",
    "            cont_names = ['age', 'fnlwgt', 'education-num'], y_names='salary')\n",
    "model_params = dict(n_d=16, n_a=16, lambda_sparse=1e-4, bs=1024*4, \n",
    "                          virtual_batch_size=128, n_steps=5, gamma=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Forest Cover DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T14:39:19.774380Z",
     "start_time": "2020-10-29T14:39:19.771616Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_dir = Path('./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T14:39:19.930529Z",
     "start_time": "2020-10-29T14:39:19.927045Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def extract_gzip(file, dest=None):\n",
    "    import gzip\n",
    "    dest = dest or Path(dest)\n",
    "    with gzip.open(file, 'rb') as f_in:\n",
    "        with open(dest / file.stem, 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T14:39:20.072621Z",
     "start_time": "2020-10-29T14:39:20.067292Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "forest_type_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz'\n",
    "forest_path = untar_data(forest_type_url, dest=data_dir, extract_func=extract_gzip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T14:39:20.663198Z",
     "start_time": "2020-10-29T14:39:20.656857Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "target = \"Covertype\"\n",
    "\n",
    "cat_names = [\n",
    "    \"Wilderness_Area1\", \"Wilderness_Area2\", \"Wilderness_Area3\",\n",
    "    \"Wilderness_Area4\", \"Soil_Type1\", \"Soil_Type2\", \"Soil_Type3\", \"Soil_Type4\",\n",
    "    \"Soil_Type5\", \"Soil_Type6\", \"Soil_Type7\", \"Soil_Type8\", \"Soil_Type9\",\n",
    "    \"Soil_Type10\", \"Soil_Type11\", \"Soil_Type12\", \"Soil_Type13\", \"Soil_Type14\",\n",
    "    \"Soil_Type15\", \"Soil_Type16\", \"Soil_Type17\", \"Soil_Type18\", \"Soil_Type19\",\n",
    "    \"Soil_Type20\", \"Soil_Type21\", \"Soil_Type22\", \"Soil_Type23\", \"Soil_Type24\",\n",
    "    \"Soil_Type25\", \"Soil_Type26\", \"Soil_Type27\", \"Soil_Type28\", \"Soil_Type29\",\n",
    "    \"Soil_Type30\", \"Soil_Type31\", \"Soil_Type32\", \"Soil_Type33\", \"Soil_Type34\",\n",
    "    \"Soil_Type35\", \"Soil_Type36\", \"Soil_Type37\", \"Soil_Type38\", \"Soil_Type39\",\n",
    "    \"Soil_Type40\"\n",
    "]\n",
    "\n",
    "cont_names = [\n",
    "    \"Elevation\", \"Aspect\", \"Slope\", \"Horizontal_Distance_To_Hydrology\",\n",
    "    \"Vertical_Distance_To_Hydrology\", \"Horizontal_Distance_To_Roadways\",\n",
    "    \"Hillshade_9am\", \"Hillshade_Noon\", \"Hillshade_3pm\",\n",
    "    \"Horizontal_Distance_To_Fire_Points\"\n",
    "]\n",
    "\n",
    "feature_columns = (\n",
    "    cont_names + cat_names + [target])\n",
    "\n",
    "params = dict(cont_names = cont_names, y_names = target, cat_names = cat_names)\n",
    "procs=[Categorify, FillMissing, Normalize]\n",
    "model_params = dict(n_d=64, n_a=64, n_steps=5, virtual_batch_size=512, gamma=1.5, bs=1024*16, lambda_sparse=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T14:39:22.653029Z",
     "start_time": "2020-10-29T14:39:21.303510Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(forest_path, header=None, names=feature_columns).sample(n=200_000)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Self Supervision + Number of Epochs\n",
    "In this experiment we'll train 2 models, one without self supervision, and one with it, and see which one does better and if number of epochs matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T19:00:28.902261Z",
     "start_time": "2020-10-28T18:54:03.198595Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture \n",
    "res = L([score_before_after_ss(df, params, model_params) for i in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T19:00:28.928345Z",
     "start_time": "2020-10-28T19:00:28.904371Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "before = res.itemgot(0).map(lambda b: accuracy(*b))\n",
    "after = res.itemgot(1).map(lambda b: accuracy(*b))\n",
    "\n",
    "pd.DataFrame({'before': before, 'after': after}).agg(['mean', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T19:00:28.902261Z",
     "start_time": "2020-10-28T18:54:03.198595Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture \n",
    "res = L([score_before_after_ss(df, params, model_params, cycle_lr=[(20, 1e-1/2)]*3) for i in range(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T19:00:28.928345Z",
     "start_time": "2020-10-28T19:00:28.904371Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "before = res.itemgot(0).map(lambda b: accuracy(*b))\n",
    "after = res.itemgot(1).map(lambda b: accuracy(*b))\n",
    "\n",
    "pd.DataFrame({'before': before, 'after': after}).agg(['mean', 'std'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Supervision + Curriculum\n",
    "In this experiment we'll train 2 models with self-supervised learning. One with curriculum learning and one without and check if it improves the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T14:58:41.427868Z",
     "start_time": "2020-10-29T14:53:06.398714Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture \n",
    "res = L([score_before_after_ss(df, params, model_params, cycle_lr=[(15, 1e-1/2)]*3) \n",
    "                             for i in range(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T14:58:41.465811Z",
     "start_time": "2020-10-29T14:58:41.430221Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "before = res.itemgot(0).map(lambda b: accuracy(*b))\n",
    "after = res.itemgot(1).map(lambda b: accuracy(*b))\n",
    "\n",
    "pd.DataFrame({'before': before, 'after': after}).agg(['mean', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T15:16:56.499064Z",
     "start_time": "2020-10-29T15:11:31.005291Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture \n",
    "res = L([score_before_after_ss(df, params, model_params, cycle_lr=[(15, 1e-1/2)]*3, curriculum=True) \n",
    "                             for i in range(2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T15:16:56.536241Z",
     "start_time": "2020-10-29T15:16:56.500990Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "before = res.itemgot(0).map(lambda b: accuracy(*b))\n",
    "after = res.itemgot(1).map(lambda b: accuracy(*b))\n",
    "\n",
    "pd.DataFrame({'before': before, 'after': after}).agg(['mean', 'std'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Supervision For Small Label Regime \n",
    "In this experiment we'll check the affect of self supervised learning on problems without a lof of labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T13:59:49.648875Z",
     "start_time": "2020-10-29T13:59:49.644529Z"
    }
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T14:11:40.512542Z",
     "start_time": "2020-10-29T14:11:39.239848Z"
    }
   },
   "outputs": [],
   "source": [
    "tp = tabular_pandas(df, **params, val_pct=0.2)\n",
    "tp.train.ys[tp.y_names[0]].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T21:49:56.756462Z",
     "start_time": "2020-10-28T21:49:55.433549Z"
    }
   },
   "outputs": [],
   "source": [
    "learn = tabnet_df_classifier(df, **params, tabnet_args=new_params, val_pct=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T22:30:41.813142Z",
     "start_time": "2020-10-28T21:49:57.773843Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(30, 1e-1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T21:33:47.793771Z",
     "start_time": "2020-10-28T21:33:46.536856Z"
    }
   },
   "outputs": [],
   "source": [
    "learn = tabnet_df_classifier(df, **params, tabnet_args=new_params, val_pct=0.995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T21:49:44.649793Z",
     "start_time": "2020-10-28T21:33:48.069600Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(30, 1e-1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T22:35:27.764867Z",
     "start_time": "2020-10-28T22:35:26.420225Z"
    }
   },
   "outputs": [],
   "source": [
    "learn_ss = tabnet_df_self_sup(df, **params, tabnet_args=model_params)\n",
    "learn_ss.dls.train.n, learn_ss.dls.valid.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T22:39:30.897747Z",
     "start_time": "2020-10-28T22:35:32.162162Z"
    }
   },
   "outputs": [],
   "source": [
    "learn_ss.fit_one_cycle(50, 1e-1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T22:39:31.849184Z",
     "start_time": "2020-10-28T22:39:31.841078Z"
    }
   },
   "outputs": [],
   "source": [
    "learn_ss.dls.ys.iloc[:,0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T22:39:49.377151Z",
     "start_time": "2020-10-28T22:39:48.138796Z"
    }
   },
   "outputs": [],
   "source": [
    "tp = tabular_pandas(df, **params, val_pct=0.995)\n",
    "tp.train.ys[tp.y_names[0]].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T22:40:08.418136Z",
     "start_time": "2020-10-28T22:40:08.415151Z"
    }
   },
   "outputs": [],
   "source": [
    "new_params = model_params.copy()\n",
    "new_params['bs'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T22:40:10.643239Z",
     "start_time": "2020-10-28T22:40:09.365440Z"
    }
   },
   "outputs": [],
   "source": [
    "learn = tabnet_df_classifier(df, **params, tabnet_args=new_params, enc=learn_ss.model.enc, val_pct=0.995)\n",
    "learn.dls.train.n, learn.dls.valid.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T22:55:55.630842Z",
     "start_time": "2020-10-28T22:40:11.071413Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(30, 1e-1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "304.475px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
