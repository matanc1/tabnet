{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T13:28:27.028825Z",
     "start_time": "2020-10-16T13:28:27.023903Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "#default_exp model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T13:28:27.623521Z",
     "start_time": "2020-10-16T13:28:27.620120Z"
    }
   },
   "outputs": [],
   "source": [
    "#exporti\n",
    "from fastai.tabular.all import * \n",
    "from tabnet.sparsemax import Sparsemax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T12:19:33.921679Z",
     "start_time": "2020-10-16T12:19:29.505613Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /home/jupyter/libraries/fastcore\n",
      "Obtaining file:///home/jupyter/libraries/fastai\n",
      "Requirement already satisfied: pip in /opt/conda/lib/python3.7/site-packages (from fastcore==1.1.3) (20.1.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from fastcore==1.1.3) (20.4)\n",
      "Requirement already satisfied: torchvision>=0.7 in /opt/conda/lib/python3.7/site-packages (from fastai==2.0.16) (0.7.0)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from fastai==2.0.16) (3.2.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from fastai==2.0.16) (1.0.5)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from fastai==2.0.16) (2.24.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from fastai==2.0.16) (5.3.1)\n",
      "Requirement already satisfied: fastprogress>=0.2.4 in /opt/conda/lib/python3.7/site-packages (from fastai==2.0.16) (1.0.0)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (from fastai==2.0.16) (7.1.2)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from fastai==2.0.16) (0.23.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from fastai==2.0.16) (1.5.0)\n",
      "Requirement already satisfied: spacy in /opt/conda/lib/python3.7/site-packages (from fastai==2.0.16) (2.1.8)\n",
      "Requirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from fastai==2.0.16) (1.6.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging->fastcore==1.1.3) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->fastcore==1.1.3) (2.4.7)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision>=0.7->fastai==2.0.16) (1.18.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->fastai==2.0.16) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->fastai==2.0.16) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->fastai==2.0.16) (1.2.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->fastai==2.0.16) (2020.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->fastai==2.0.16) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->fastai==2.0.16) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->fastai==2.0.16) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->fastai==2.0.16) (1.25.9)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->fastai==2.0.16) (0.15.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->fastai==2.0.16) (2.1.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy->fastai==2.0.16) (1.0.0)\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from spacy->fastai==2.0.16) (2.0.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /opt/conda/lib/python3.7/site-packages (from spacy->fastai==2.0.16) (0.2.2)\n",
      "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /opt/conda/lib/python3.7/site-packages (from spacy->fastai==2.0.16) (7.0.8)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /opt/conda/lib/python3.7/site-packages (from spacy->fastai==2.0.16) (0.1.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy->fastai==2.0.16) (2.0.2)\n",
      "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /opt/conda/lib/python3.7/site-packages (from spacy->fastai==2.0.16) (0.2.4)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /opt/conda/lib/python3.7/site-packages (from spacy->fastai==2.0.16) (0.9.6)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch>=1.6.0->fastai==2.0.16) (0.18.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /opt/conda/lib/python3.7/site-packages (from thinc<7.1.0,>=7.0.8->spacy->fastai==2.0.16) (4.47.0)\n",
      "Building wheels for collected packages: fastcore\n",
      "  Building wheel for fastcore (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fastcore: filename=fastcore-1.1.3-py3-none-any.whl size=43299 sha256=4ddad464f8e42683a92d4fd7a5da5c7c50ef95edbc21e89baee6116c30eb48bd\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-08qajf4h/wheels/f5/2c/77/44ecc13f884dd9f69af0b9d1cffa0421c2b261c7e7ff267cbd\n",
      "Successfully built fastcore\n",
      "Installing collected packages: fastcore, fastai\n",
      "  Attempting uninstall: fastcore\n",
      "    Found existing installation: fastcore 1.1.3\n",
      "    Uninstalling fastcore-1.1.3:\n",
      "      Successfully uninstalled fastcore-1.1.3\n",
      "  Attempting uninstall: fastai\n",
      "    Found existing installation: fastai 2.0.16\n",
      "    Uninstalling fastai-2.0.16:\n",
      "      Successfully uninstalled fastai-2.0.16\n",
      "  Running setup.py develop for fastai\n",
      "Successfully installed fastai fastcore-1.1.3\n"
     ]
    }
   ],
   "source": [
    "! pip install -e ../../libraries/fastai ../../libraries/fastcore "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T13:28:29.690406Z",
     "start_time": "2020-10-16T13:28:29.684462Z"
    }
   },
   "outputs": [],
   "source": [
    "class GBN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "        Ghost Batch Normalization\n",
    "        https://arxiv.org/abs/1705.08741\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, virtual_batch_size=128, momentum=0.02):\n",
    "        super(GBN, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.bn = BatchNorm(self.input_dim, momentum=momentum, ndim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        chunks = x.chunk(int(np.ceil(x.shape[0] / self.virtual_batch_size)), 0)\n",
    "        res = [self.bn(x_) for x_ in chunks]\n",
    "\n",
    "        return torch.cat(res, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T13:48:38.435456Z",
     "start_time": "2020-10-16T13:48:38.421404Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class TabNet(Module):\n",
    "    \n",
    "    def __init__(self, emb_szs, n_cont, out_features, n_d, n_a, n_steps, n_shared_ft_blocks=2,\n",
    "                         n_independent_ft_blocks=2, gamma=1.5, virtual_batch_size=128):        \n",
    "        store_attr()\n",
    "        \n",
    "        self.embeds = nn.ModuleList([Embedding(ni, nf) for ni,nf in emb_szs])\n",
    "        self.n_emb = sum(e.embedding_dim for e in self.embeds)\n",
    "        self.n_features = self.n_emb + n_cont\n",
    "        \n",
    "        \n",
    "        intermediate_features = n_d + n_a \n",
    "        self.shared_ft_blocks = [FeatureTransformerBlock(self.n_features, intermediate_features, virtual_batch_size, \n",
    "                                                         is_first=True)] + \\\n",
    "                                [FeatureTransformerBlock(intermediate_features, intermediate_features, virtual_batch_size,\n",
    "                                                         is_first=False) for _ in range(n_shared_ft_blocks)]\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.initial_bn = BatchNorm(self.n_features, ndim=1)\n",
    "        self.initial_ft = FeatureTransformer(self.shared_ft_blocks, n_d, n_a, n_independent_ft_blocks, virtual_batch_size)\n",
    "        \n",
    "        self.att_steps = nn.ModuleList([AttentiveTransformer(n_a, self.n_features,virtual_batch_size) \n",
    "                                            for i in range(self.n_steps)])\n",
    "        self.ft_steps = nn.ModuleList([FeatureTransformer(self.shared_ft_blocks, n_d, n_a,\n",
    "                                                          n_independent_ft_blocks, virtual_batch_size) \n",
    "                                            for i in range(self.n_steps)])\n",
    "        \n",
    "        self.final_fc = nn.Linear(n_d, out_features)\n",
    "    \n",
    "    def forward(self, x_cat, x_cont):\n",
    "        x = self._combine_cat_cont(x_cat, x_cont)\n",
    "        \n",
    "        output = 0\n",
    "        x = self.initial_bn(x)\n",
    "        _, a = self.initial_ft(x)\n",
    "        \n",
    "        prior = torch.ones(self.n_features, device=x_cont.device)\n",
    "        \n",
    "        for i in range(self.n_steps):\n",
    "            M = self.att_steps[i](a, prior)\n",
    "            prior = (self.gamma - M)*prior\n",
    "            res = M * x\n",
    "            d, a = self.ft_steps[i](res)\n",
    "            output = output + nn.functional.relu(d)\n",
    "        \n",
    "        res = self.final_fc(output)\n",
    "        return res\n",
    "    \n",
    "    def _combine_cat_cont(self, x_cat, x_cont):\n",
    "        if self.n_emb != 0:\n",
    "            x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)]\n",
    "            x = torch.cat(x, 1)\n",
    "        if self.n_cont != 0:\n",
    "            x = torch.cat([x, x_cont], 1) if self.n_emb != 0 else x_cont\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T13:48:38.976645Z",
     "start_time": "2020-10-16T13:48:38.971736Z"
    }
   },
   "outputs": [],
   "source": [
    "class AttentiveTransformer(Module):\n",
    "    \n",
    "    def __init__(self, n_a, in_features, virtual_batch_size):\n",
    "        store_attr()\n",
    "        self.fc = nn.Linear(n_a, in_features)\n",
    "        self.bn = GBN(in_features, virtual_batch_size)\n",
    "        self.sparsemax = Sparsemax()\n",
    "        \n",
    "    def forward(self, a, prior):\n",
    "        a = self.fc(a)\n",
    "        a = self.bn(a)\n",
    "        a = prior * a\n",
    "        M = self.sparsemax(a)\n",
    "        return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T13:48:39.290739Z",
     "start_time": "2020-10-16T13:48:39.285335Z"
    }
   },
   "outputs": [],
   "source": [
    "class FeatureTransformer(Module):\n",
    "    \n",
    "    def __init__(self, shared_blocks, n_d, n_a, n_independent_ft_blocks, virtual_batch_size):\n",
    "        store_attr()\n",
    "        intermediate_features = n_d + n_a\n",
    "        steps = [FeatureTransformerBlock(intermediate_features, intermediate_features, virtual_batch_size, False) \n",
    "                             for _ in range(n_independent_ft_blocks)]\n",
    "        self.steps = nn.Sequential(*[*shared_blocks, *steps])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        res = self.steps(x)\n",
    "        d, a = res[:,:self.n_d], res[:,self.n_d:]\n",
    "        return d, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T13:48:39.482290Z",
     "start_time": "2020-10-16T13:48:39.475163Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class FeatureTransformerBlock(Module):\n",
    "    def __init__(self, in_features, intermediate_features, virtual_batch_size, is_first, norm=math.sqrt(0.5)):\n",
    "        store_attr()\n",
    "        \n",
    "        self.block1 = self._create_inner_block(is_first=True)\n",
    "        self.block2 = self._create_inner_block(is_first=False)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.block1(x)\n",
    "        if not self.is_first: x1 = (x1 + x)*self.norm\n",
    "        x2 = self.block2(x1)\n",
    "        x2 = (x2 + x1)*self.norm\n",
    "        return x2\n",
    "        \n",
    "        \n",
    "    def _create_inner_block(self, is_first):\n",
    "        intermediate_features = self.intermediate_features\n",
    "        in_features = self.in_features if is_first else intermediate_features\n",
    "        \n",
    "        return nn.Sequential(*[\n",
    "            nn.Linear(in_features, 2*intermediate_features),\n",
    "            GBN(2*intermediate_features, self.virtual_batch_size),\n",
    "            nn.GLU(),\n",
    "        ])        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T13:48:39.636332Z",
     "start_time": "2020-10-16T13:48:39.627318Z"
    }
   },
   "outputs": [],
   "source": [
    "N = 3\n",
    "n_features = 32\n",
    "n_d = n_a = 7 \n",
    "n_steps = 3\n",
    "out_features = 10\n",
    "virtual_batch_size = 5\n",
    "\n",
    "a = torch.randn((N, n_features))\n",
    "ft = FeatureTransformerBlock(n_features, n_d+n_a, virtual_batch_size, is_first=True)\n",
    "test_eq(ft(a).shape, (N, n_d+n_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T13:48:39.841821Z",
     "start_time": "2020-10-16T13:48:39.814494Z"
    }
   },
   "outputs": [],
   "source": [
    "a = torch.randn((N, n_features))\n",
    "tabnet = TabNet([], n_features, out_features, n_d, n_a, n_steps, virtual_batch_size=virtual_batch_size)\n",
    "test_eq(tabnet(a, a).shape, (N, out_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing it out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T13:48:40.214263Z",
     "start_time": "2020-10-16T13:48:40.211172Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = Path('./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T13:48:40.668498Z",
     "start_time": "2020-10-16T13:48:40.664283Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_gzip(file, dest=None):\n",
    "    import gzip\n",
    "    dest = dest or Path(dest)\n",
    "    with gzip.open(file, 'rb') as f_in:\n",
    "        with open(dest / file.stem, 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T13:48:41.010547Z",
     "start_time": "2020-10-16T13:48:41.004908Z"
    }
   },
   "outputs": [],
   "source": [
    "forest_type_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz'\n",
    "forest_path = untar_data(forest_type_url, dest=data_dir, extract_func=extract_gzip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T13:48:41.186241Z",
     "start_time": "2020-10-16T13:48:41.180661Z"
    }
   },
   "outputs": [],
   "source": [
    "target = \"Covertype\"\n",
    "\n",
    "cat_names = [\n",
    "    \"Wilderness_Area1\", \"Wilderness_Area2\", \"Wilderness_Area3\",\n",
    "    \"Wilderness_Area4\", \"Soil_Type1\", \"Soil_Type2\", \"Soil_Type3\", \"Soil_Type4\",\n",
    "    \"Soil_Type5\", \"Soil_Type6\", \"Soil_Type7\", \"Soil_Type8\", \"Soil_Type9\",\n",
    "    \"Soil_Type10\", \"Soil_Type11\", \"Soil_Type12\", \"Soil_Type13\", \"Soil_Type14\",\n",
    "    \"Soil_Type15\", \"Soil_Type16\", \"Soil_Type17\", \"Soil_Type18\", \"Soil_Type19\",\n",
    "    \"Soil_Type20\", \"Soil_Type21\", \"Soil_Type22\", \"Soil_Type23\", \"Soil_Type24\",\n",
    "    \"Soil_Type25\", \"Soil_Type26\", \"Soil_Type27\", \"Soil_Type28\", \"Soil_Type29\",\n",
    "    \"Soil_Type30\", \"Soil_Type31\", \"Soil_Type32\", \"Soil_Type33\", \"Soil_Type34\",\n",
    "    \"Soil_Type35\", \"Soil_Type36\", \"Soil_Type37\", \"Soil_Type38\", \"Soil_Type39\",\n",
    "    \"Soil_Type40\"\n",
    "]\n",
    "\n",
    "cont_names = [\n",
    "    \"Elevation\", \"Aspect\", \"Slope\", \"Horizontal_Distance_To_Hydrology\",\n",
    "    \"Vertical_Distance_To_Hydrology\", \"Horizontal_Distance_To_Roadways\",\n",
    "    \"Hillshade_9am\", \"Hillshade_Noon\", \"Hillshade_3pm\",\n",
    "    \"Horizontal_Distance_To_Fire_Points\"\n",
    "]\n",
    "\n",
    "feature_columns = (\n",
    "    cont_names + cat_names + [target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T13:48:42.546539Z",
     "start_time": "2020-10-16T13:48:41.339728Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(forest_path, header=None, names=feature_columns); df.head()\n",
    "df = df_shrink(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T13:48:44.304191Z",
     "start_time": "2020-10-16T13:48:42.548321Z"
    }
   },
   "outputs": [],
   "source": [
    "procs = [Categorify, FillMissing, Normalize]\n",
    "splits = RandomSplitter(0.05)(range_of(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T13:48:46.338677Z",
     "start_time": "2020-10-16T13:48:44.306260Z"
    }
   },
   "outputs": [],
   "source": [
    "to = TabularPandas(df, procs, cat_names, cont_names, y_names=target, y_block = CategoryBlock(), splits=splits)\n",
    "dls = to.dataloaders(bs=64*64*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T13:48:46.376126Z",
     "start_time": "2020-10-16T13:48:46.340310Z"
    }
   },
   "outputs": [],
   "source": [
    "model = TabNet(get_emb_sz(to), len(to.cont_names), dls.c, n_d=64, n_a=64, n_steps=5, virtual_batch_size=256)\n",
    "opt_func = partial(Adam, wd=0.01, eps=1e-5)\n",
    "learn = Learner(dls, model, CrossEntropyLossFlat(), opt_func=opt_func, lr=3e-2, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T13:52:55.411445Z",
     "start_time": "2020-10-16T13:48:46.442153Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.338366</td>\n",
       "      <td>0.884798</td>\n",
       "      <td>0.671050</td>\n",
       "      <td>00:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.984640</td>\n",
       "      <td>0.768904</td>\n",
       "      <td>0.672220</td>\n",
       "      <td>00:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.854257</td>\n",
       "      <td>0.862537</td>\n",
       "      <td>0.645473</td>\n",
       "      <td>00:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.783051</td>\n",
       "      <td>0.769091</td>\n",
       "      <td>0.664750</td>\n",
       "      <td>00:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.741406</td>\n",
       "      <td>0.788258</td>\n",
       "      <td>0.664131</td>\n",
       "      <td>00:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.712884</td>\n",
       "      <td>0.787519</td>\n",
       "      <td>0.662788</td>\n",
       "      <td>00:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.693834</td>\n",
       "      <td>0.859104</td>\n",
       "      <td>0.656730</td>\n",
       "      <td>00:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.681376</td>\n",
       "      <td>0.887326</td>\n",
       "      <td>0.644819</td>\n",
       "      <td>00:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.673750</td>\n",
       "      <td>0.852126</td>\n",
       "      <td>0.640516</td>\n",
       "      <td>00:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.668546</td>\n",
       "      <td>0.864051</td>\n",
       "      <td>0.649776</td>\n",
       "      <td>00:24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T14:05:21.568216Z",
     "start_time": "2020-10-13T15:50:17.883Z"
    }
   },
   "outputs": [],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "304.475px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
