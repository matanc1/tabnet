{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T14:30:54.957875Z",
     "start_time": "2020-10-31T14:30:54.940518Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "#default_exp model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T14:30:55.777900Z",
     "start_time": "2020-10-31T14:30:54.959511Z"
    }
   },
   "outputs": [],
   "source": [
    "#exporti\n",
    "from fastai.tabular.all import * \n",
    "from tabnet.core import Sparsemax, GBN\n",
    "from mock import Mock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TabNet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T14:30:55.785957Z",
     "start_time": "2020-10-31T14:30:55.779739Z"
    }
   },
   "outputs": [],
   "source": [
    "#exporti\n",
    "def _initial_block(n_in, n_out):\n",
    "    return nn.Linear(n_in, 2*n_out, bias=False)\n",
    "\n",
    "def _rest_block(n):\n",
    "    return nn.Linear(n, 2*n, bias=False)\n",
    "\n",
    "def _create_shared_blocks(n_in, n_out, n_shared):\n",
    "    return [_initial_block(n_in, n_out)] + \\\n",
    "            [_rest_block(n_out) for _ in range(n_shared-1)]\n",
    "\n",
    "def _combine_cat_cont(x_cat, x_cont, embeds):\n",
    "    if x_cat.shape[1] != 0: \n",
    "        x = [e(x_cat[:,i]) for i,e in enumerate(embeds)]\n",
    "        x_cat = torch.cat(x, 1)\n",
    "\n",
    "    x = torch.cat([x_cat, x_cont], 1)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T14:30:55.792341Z",
     "start_time": "2020-10-31T14:30:55.787763Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class TabNetBase(Module):\n",
    "    def __init__(self, n_d, n_a, n_steps, n_shared_ft_blocks, n_independent_ft_blocks, \n",
    "                 virtual_batch_size, momentum, **kwargs):\n",
    "        store_attr()\n",
    "    \n",
    "    \n",
    "    def _create_feature_transform(self, shared_ft_blocks):\n",
    "        return FeatureTransformer(self.n_d+self.n_a, shared_ft_blocks,\n",
    "                                          self.n_independent_ft_blocks,\n",
    "                                          self.virtual_batch_size, self.momentum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Notice the prior in the `forward` method. It's set by the `SetPrior` callback later. I've put it in a callback as we can choose different types of prior methods. \n",
    "* Notice that the output of the encoder is the stacked outputs of each step (we don't assume anything for the later layers). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T16:43:10.774958Z",
     "start_time": "2020-10-31T16:43:10.763996Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#exporti\n",
    "class TabNetEnc(TabNetBase):\n",
    "    \n",
    "    @delegates(TabNetBase.__init__)\n",
    "    def __init__(self, emb_szs, n_cont, gamma, **kwargs):        \n",
    "        store_attr()\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.embeds = nn.ModuleList([Embedding(ni, nf) for ni,nf in self.emb_szs])\n",
    "        self.n_emb = sum(e.embedding_dim for e in self.embeds)\n",
    "        self.n_features = self.n_emb + self.n_cont\n",
    "        \n",
    "        shared_ft_blocks = _create_shared_blocks(self.n_features, self.n_d + self.n_a, self.n_shared_ft_blocks)\n",
    "        \n",
    "        self.initial_ft = self._create_feature_transform(shared_ft_blocks)        \n",
    "        self.initial_bn = BatchNorm(self.n_features, ndim=1)    \n",
    "        \n",
    "        self.att_steps = nn.ModuleList([AttentiveTransformer(self.n_a, self.n_features,self.virtual_batch_size, \n",
    "                                                             self.momentum) for i in range(self.n_steps)])\n",
    "        \n",
    "        self.ft_steps = nn.ModuleList([self._create_feature_transform(shared_ft_blocks) for _ in range(self.n_steps)])        \n",
    "    \n",
    "    \n",
    "    def _split(self, x):\n",
    "        return x[:, :self.n_d], x[:, self.n_d:]\n",
    "    \n",
    "    def forward(self, x_cat, x_cont):\n",
    "        x = _combine_cat_cont(x_cat, x_cont, self.embeds)\n",
    "\n",
    "        x = x * self.prior \n",
    "        \n",
    "        output = []\n",
    "        x = self.initial_bn(x)\n",
    "        res = self.initial_ft(x)\n",
    "        d, a = self._split(res)\n",
    "        \n",
    "        self.masks = []\n",
    "        for i in range(self.n_steps):\n",
    "            M = self.att_steps[i](self.prior, a)\n",
    "            self.masks.append(M)\n",
    "            self.prior = (self.gamma - M)*self.prior\n",
    "            masked_x = M * x\n",
    "            transformed_x = self.ft_steps[i](masked_x)\n",
    "            d, a = self._split(transformed_x)\n",
    "            output.append(nn.functional.relu(d))\n",
    "        \n",
    "        return torch.stack(output, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T16:26:31.406541Z",
     "start_time": "2020-10-31T16:26:31.398022Z"
    }
   },
   "outputs": [],
   "source": [
    "#exporti\n",
    "class FeatureTransformer(Module):\n",
    "    def __init__(self, n_out, shared_layers, n_indep,\n",
    "                 virtual_batch_size=128, momentum=0.02):\n",
    "        store_attr()\n",
    "        \n",
    "        shared_layers = [FeatureTransformerBlock(layer, n_out, virtual_batch_size, momentum) \n",
    "                             for layer in shared_layers]\n",
    "        independent_layers = [FeatureTransformerBlock(_rest_block(n_out), n_out, virtual_batch_size, momentum) \n",
    "                              for _ in range(n_indep)]\n",
    "        \n",
    "        self.layers = nn.ModuleList([*shared_layers, *independent_layers])\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        scale = torch.sqrt(torch.FloatTensor([0.5]).to(x.device))\n",
    "        x = self.layers[0](x)\n",
    "        \n",
    "        for layer in self.layers[1:]:\n",
    "            x = scale * (x+layer(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "class FeatureTransformerBlock(nn.Sequential):\n",
    "    def __init__(self, fc, n_out,\n",
    "                 virtual_batch_size=128, momentum=0.02):\n",
    "        self.n_out = n_out\n",
    "        \n",
    "        layers = [\n",
    "                    fc, \n",
    "                    GBN(2*n_out, virtual_batch_size=virtual_batch_size, momentum=momentum),\n",
    "                    nn.GLU()\n",
    "                 ]\n",
    "        super().__init__(*layers)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T16:26:31.660656Z",
     "start_time": "2020-10-31T16:26:31.655541Z"
    }
   },
   "outputs": [],
   "source": [
    "#exporti\n",
    "class AttentiveTransformer(Module):\n",
    "    \n",
    "    def __init__(self, n_in, n_out, virtual_batch_size, momentum):\n",
    "        store_attr()\n",
    "        self.fc = nn.Linear(n_in, n_out)\n",
    "        self.bn = GBN(n_out, virtual_batch_size, momentum)\n",
    "        self.sparsemax = Sparsemax()\n",
    "        \n",
    "    def forward(self, prior, a):\n",
    "        a = self.fc(a)\n",
    "        a = self.bn(a)\n",
    "        a = prior * a\n",
    "        M = self.sparsemax(a)\n",
    "        return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T14:30:55.894708Z",
     "start_time": "2020-10-31T14:30:55.850098Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N = 3\n",
    "n_cat = 5\n",
    "n_cont = 10\n",
    "n_d = n_a = 7 \n",
    "n_steps = 4\n",
    "n_out = 10\n",
    "virtual_batch_size = 5\n",
    "\n",
    "x_cont = torch.randn((N, 10))\n",
    "x_cat = torch.randint(high=3, size=(N, n_cat))\n",
    "enc = TabNetEnc([(3, 10)]*n_cat, n_cont, n_d=n_d, n_a=n_a, n_steps=n_steps)\n",
    "enc.prior = 1 \n",
    "test_eq(enc(x_cat, x_cont).shape, (N, n_steps, n_d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Classifier Head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work with the regular CE loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T17:31:38.274674Z",
     "start_time": "2020-10-31T17:31:38.269405Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(TabNetBase.__init__)\n",
    "def TabNet(head_func, emb_szs, n_cont, n_out, **kwargs):    \n",
    "    class TabNetWithHead(Module): \n",
    "        def __init__(self, enc, head): store_attr()\n",
    "        def forward(self, x_cat, x_cont): return self.head(self.enc(x_cat, x_cont)) \n",
    "    \n",
    "    return TabNetWithHead(TabNetEnc(emb_szs, n_cont, **kwargs), head_func(len(emb_szs), n_cont, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T17:31:38.718016Z",
     "start_time": "2020-10-31T17:31:38.712239Z"
    }
   },
   "outputs": [],
   "source": [
    "#export \n",
    "def linear_head(n_cat, n_cont, n_d, **kwargs):\n",
    "    \n",
    "    class SimpleLinearHead(Module):\n",
    "        def __init__(self, n_cat, n_cont, n_d, **kwargs):\n",
    "            self.fc_cat = nn.Linear(n_d, n_cat)\n",
    "            self.fc_cont = nn.Linear(n_d, n_cont)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x_cat = self.fc_cat(x)\n",
    "            x_cont = self.fc_cont(x)\n",
    "            \n",
    "            return torch.cat([x_cat, x_cont], dim=1)\n",
    "    \n",
    "    \n",
    "    return nn.Sequential(Lambda(lambda x: x.sum(dim=1)), SimpleLinearHead(n_cat, n_cont, n_d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T17:25:46.634590Z",
     "start_time": "2020-10-31T17:25:46.607758Z"
    }
   },
   "outputs": [],
   "source": [
    "classifier = TabNet(head_func=linear_head, emb_szs=[(3, 10)]*n_cat, n_cont=n_cont, n_out=n_out, \n",
    "                          n_steps=n_steps, n_d=n_d, n_a=n_a, virtual_batch_size=virtual_batch_size)\n",
    "classifier.enc.prior = 1 \n",
    "test_eq(classifier(x_cat, x_cont).shape, (N, n_cont+n_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Mask Regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T14:30:55.945159Z",
     "start_time": "2020-10-31T14:30:55.938796Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class MaskRegularizer(Callback):\n",
    "    def __init__(self, lambda_sparse=1e-4, eps=1e-6):\n",
    "        store_attr()\n",
    "        self.loss_regs = []\n",
    "    \n",
    "    def after_loss(self):\n",
    "        masks = self.model.enc.masks\n",
    "        Ms = torch.stack(masks)\n",
    "        n_steps,B,_ = Ms.shape\n",
    "        res = ((Ms + self.eps).log()*(-Ms)/(n_steps * B)).sum()\n",
    "        self.loss_regs.append(res)\n",
    "        self.learn.loss = self.loss + self.lambda_sparse*res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T14:30:55.950493Z",
     "start_time": "2020-10-31T14:30:55.946986Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class SetPrior(Callback):\n",
    "    def before_batch(self):\n",
    "        self.learn.model.enc.prior = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Supervision "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll try 2 methods for SS that correspond to 2 different loss functions:\n",
    "1. `MaskReconstructionLoss` as was proposed in the paper\n",
    "1. `MyLoss` which is a mix of CE for the categorical variables, MAE for the numeric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper Decoder + Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For self-supervision, we need to create a decoder. \n",
    "The decoder receives the `x`s in a (instance_index, step_index, step_result) fashion. \n",
    "Need to chunk it to get (i) batches which correspond to the (i-th) step's output. i.e ((step_index, instance_index, step_result))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T14:45:16.562106Z",
     "start_time": "2020-10-31T14:45:16.555072Z"
    }
   },
   "outputs": [],
   "source": [
    "#exporti\n",
    "class TabNetDec(TabNetBase):\n",
    "    \n",
    "    @delegates(TabNetBase.__init__)\n",
    "    def __init__(self, n_out, **kwargs):\n",
    "        store_attr()\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        shared_ft_blocks = _create_shared_blocks(self.n_d, self.n_d + self.n_a, self.n_shared_ft_blocks)\n",
    "        \n",
    "        self.steps = nn.ModuleList([\n",
    "                            nn.Sequential(\n",
    "                                self._create_feature_transform(shared_ft_blocks),\n",
    "                                nn.Linear(self.n_d+self.n_a, self.n_out)) for _ in range(self.n_steps)\n",
    "                        ])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        xs = x.chunk(self.n_steps, dim=1)\n",
    "        xs = [x.squeeze(1) for x in xs] #squeeze to remove the extra \"chunk\" dimension\n",
    "        \n",
    "        output = 0 \n",
    "        \n",
    "        for x,step in zip(xs, self.steps): \n",
    "            output = output + step(x)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T14:45:16.746867Z",
     "start_time": "2020-10-31T14:45:16.719245Z"
    }
   },
   "outputs": [],
   "source": [
    "dec = TabNetDec(n_cont+n_cat, n_steps=n_steps, n_d=n_d, n_a=n_a, virtual_batch_size=virtual_batch_size)\n",
    "test_eq(dec(enc(x_cat, x_cont)).shape, (N, n_cont+n_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we get a column of features that are all equal. When that happens, the `norm` becomes 0 -> divide by 0 -> `inf` loss. \n",
    "We'll fix that by removing very small values (close to 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T18:27:10.768089Z",
     "start_time": "2020-10-31T18:27:10.761606Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class MaskReconstructionLoss(Module):\n",
    "    def __init__(self, lambda_reg=1e-4, eps=1e-5): store_attr()\n",
    "    \n",
    "    def forward(self, preds, targ):\n",
    "        preds, targ = preds*(1-self.S), targ*(1-self.S)\n",
    "        norm = (targ - targ.mean(dim=0)).pow(2).sum(dim=0).sqrt()\n",
    "        norm_mask = norm >= 1e-6\n",
    "        norm = norm[norm_mask]\n",
    "        error = (preds - targ)\n",
    "        error = error[:,norm_mask]\n",
    "\n",
    "        loss = (error / norm).pow(2).sum(dim=1).mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T18:27:11.968741Z",
     "start_time": "2020-10-31T18:27:11.961190Z"
    }
   },
   "outputs": [],
   "source": [
    "a = tensor([[1,2],[3,4],[5,6]], dtype=float).requires_grad_()\n",
    "b = tensor([[1,2],[3,3.8],[5.2,6]], dtype=float).requires_grad_()\n",
    "\n",
    "loss_func = MaskReconstructionLoss()\n",
    "loss_func.S = torch.ones_like(a)\n",
    "loss = loss_func(a,b)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder + Decoder Head = Self Supervised Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T14:30:56.024818Z",
     "start_time": "2020-10-31T14:30:56.021816Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def tabnet_decoder(n_out, **kwargs): return TabNetDec(n_out, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T14:30:56.062099Z",
     "start_time": "2020-10-31T14:30:56.026650Z"
    }
   },
   "outputs": [],
   "source": [
    "tbss = TabNet(head_func=tabnet_decoder, emb_szs=[(3, 10)]*n_cat, n_cont=n_cont, n_out=n_cat+n_cont, \n",
    "                          n_steps=n_steps, n_d=n_d, n_a=n_a, virtual_batch_size=virtual_batch_size)\n",
    "tbss.enc.prior = 1 \n",
    "test_eq(tbss(x_cat, x_cont).shape, (N, n_cat+n_cont))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask Generator Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T14:30:56.068607Z",
     "start_time": "2020-10-31T14:30:56.063825Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def create_mask(size, n_cols):\n",
    "    if n_cols is 0: return torch.ones(size)\n",
    "    rand_mat = torch.rand(*size)    \n",
    "    k_th_quant = torch.topk(rand_mat, n_cols, largest = True)[0][:,None, -1]  \n",
    "    M = rand_mat < k_th_quant\n",
    "    return M.int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T18:30:20.608092Z",
     "start_time": "2020-10-31T18:30:20.599865Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class TabularMasking(Callback):\n",
    "    \n",
    "    def __init__(self, p=0.5, curriculum=True): \n",
    "        store_attr()\n",
    "    \n",
    "    def before_batch(self):\n",
    "        x_cat, x_cont = self.xb\n",
    "        xb = torch.cat([x_cat, x_cont], dim=1)\n",
    "        n_cols = xb.shape[1]\n",
    "        if self.curriculum: \n",
    "            n_masked = (torch.linspace(0, self.p, steps=self.n_epoch)*n_cols).floor()[self.epoch].int().item()\n",
    "        else:\n",
    "            n_masked = torch.distributions.Binomial(n_cols, probs=self.p).sample().int().item()\n",
    "        self.S = to_device(create_mask(xb.shape, n_masked), x_cat.device)\n",
    "        xb = xb * self.S\n",
    "        self.learn.xb = (xb[:, :x_cat.shape[1]].long(), xb[:, x_cat.shape[1]:])\n",
    "        self.learn.loss_func.S = self.S \n",
    "        \n",
    "        y_cat, y_cont = self.yb\n",
    "        self.learn.yb = tuplify(torch.cat([y_cat, y_cont], dim=1))\n",
    "        \n",
    "    def after_pred(self):\n",
    "        pass\n",
    "#         y_cat, y_cont = self.yb\n",
    "#         self.learn.yb = tuplify(torch.cat([y_cat, y_cont], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T14:30:56.088917Z",
     "start_time": "2020-10-31T14:30:56.081805Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/libraries/fastai/fastai/callback/core.py:50: UserWarning: You are setting an attribute (S) that also exists in the learner, so you're not setting it in the learner but in the callback. Use `self.learn.S` otherwise.\n",
      "  warn(f\"You are setting an attribute ({name}) that also exists in the learner, so you're not setting it in the learner but in the callback. Use `self.learn.{name}` otherwise.\")\n"
     ]
    }
   ],
   "source": [
    "tm = TabularMasking(0.5)\n",
    "learn = Mock()\n",
    "learn.xb = torch.ones((7, 2)), torch.ones((7, 2))\n",
    "learn.n_epoch = 5\n",
    "learn.epoch = 2\n",
    "learn.loss_func = Mock()\n",
    "learn.model = Mock()\n",
    "tm.learn = learn \n",
    "tm.before_batch()\n",
    "test_eq(learn.loss_func.S.shape, (7,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T18:30:23.866257Z",
     "start_time": "2020-10-31T18:30:23.719140Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_core.ipynb.\n",
      "Converted 02_model.ipynb.\n",
      "Converted 03_experiments.ipynb.\n",
      "Converted 04_utils.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted self_supervision.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "304.475px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
