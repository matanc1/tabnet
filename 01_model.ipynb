{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-03T22:03:30.990256Z",
     "start_time": "2020-10-03T22:03:30.956370Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Cells will be exported to nbdev_template.tabnet,\n",
      "unless a different module is specified after an export flag: `%nbdev_export special.module`\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "%nbdev_default_export tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-03T21:30:03.279997Z",
     "start_time": "2020-10-03T21:30:00.622203Z"
    }
   },
   "outputs": [],
   "source": [
    "from fastai.tabular.all import * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the model \n",
    "\n",
    "The TabNet model consists of the following parts (which we'll implement):\n",
    "1. Attentive Transformer - aka, the `MaskBlock` which is in charge of creating the mask given the previous step's output\n",
    "1. Feature Transformer - which given the masked features is in charge of processing them to a better representation. Internally it's made out of multiple `FeatureTransformerBlock`s. \n",
    "\n",
    "These two are the building blocks for the `TabNetStep` which is represents the steps in a `TabNet`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo: \n",
    "1. Use `ModuleDict` \n",
    "1. Use GLU\n",
    "1. sqrt(0.5) \n",
    "1. save masks for interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mask block creates a mask from the vector that it gets from the previous step (of size, `n_transformed_features`), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-03T22:10:52.629401Z",
     "start_time": "2020-10-03T22:10:52.613034Z"
    }
   },
   "outputs": [],
   "source": [
    "#exporti \n",
    "\n",
    "class MaskBlock(Module):\n",
    "    \n",
    "    def __init__(self, n_features, n_transformed_features, max_func=nn.Softmax(dim=1), gamma=1):\n",
    "        store_attr('gamma,max_func')\n",
    "        \n",
    "        self.fc = LinBnDrop(n_transformed_features, n_features, lin_first=True)\n",
    "        self.prior = torch.ones(n_features, dtype=torch.float)\n",
    "        \n",
    "    def forward(self, prev):\n",
    "        mask = self._calc_mask(prev)\n",
    "        self.prior = self._calc_prior_scales(mask)\n",
    "        return mask\n",
    "        \n",
    "    def _calc_mask(self, prev):\n",
    "        return self.max_func(self.fc(prev) * self.prior)\n",
    "    \n",
    "    def _calc_prior_scales(self, mask):\n",
    "        return (self.gamma - mask)*self.prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 3\n",
    "n_features = 32\n",
    "n_transformed_features = 8 \n",
    "mb = MaskBlock(n_features, n_transformed_features)\n",
    "a = torch.randn((n_samples,n_transformed_features), dtype=torch.float)\n",
    "test_close(mb(a).sum(dim=1),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `FeatureTransformerBlock` uses a GLU in the end which halves the size of the input. For us to get the same size output we need the linear layers to double the output size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-03T22:10:15.869535Z",
     "start_time": "2020-10-03T22:10:15.855498Z"
    }
   },
   "outputs": [],
   "source": [
    "#exporti \n",
    "\n",
    "class FeatureTransformerBlock(Module):\n",
    "    \n",
    "    def __init__(self, n_features, n_transformed_features, add_input=False):\n",
    "        store_attr('n_features,n_transformed_features,add_input')\n",
    "        \n",
    "        self.b1 = self._inner_block(self.n_features, 2*self.n_transformed_features)\n",
    "        self.b2 = self._inner_block(self.n_transformed_features, 2*self.n_transformed_features)\n",
    "        \n",
    "\n",
    "    def forward(self, inp):\n",
    "        x1 = self.b1(inp)\n",
    "        if self.add_input: x1 += inp\n",
    "        x2 = self.b2(x1)\n",
    "        return x1 + x2\n",
    "        \n",
    "    \n",
    "    def _inner_block(self, n_in, n_out):\n",
    "        return nn.Sequential(LinBnDrop(n_in, n_out, lin_first=True), nn.GLU())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-03T22:10:16.271291Z",
     "start_time": "2020-10-03T22:10:16.248949Z"
    }
   },
   "outputs": [],
   "source": [
    "a = torch.randn((n_samples,n_features), dtype=torch.float)\n",
    "ft = FeatureTransformerBlock(n_features, n_transformed_features)\n",
    "test_eq(ft(a).shape, (n_samples, n_transformed_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-03T22:10:16.474902Z",
     "start_time": "2020-10-03T22:10:16.459032Z"
    }
   },
   "outputs": [],
   "source": [
    "#exporti \n",
    "\n",
    "class FeatureTransformer(nn.Sequential):\n",
    "    \n",
    "    def __init__(self, n_features, n_transformed_features, n_trans_blocks=2):\n",
    "        FeatureTransformer.shared_block = self._create_block(n_features, n_transformed_features)\n",
    "        layers = [\n",
    "            FeatureTransformer.shared_block, \n",
    "            *[self._create_block(n_transformed_features, n_transformed_features, add_input=True) \n",
    "                                                                      for i in range(n_trans_blocks-1)]\n",
    "        ]\n",
    "\n",
    "        super().__init__(*layers)\n",
    "        \n",
    "        \n",
    "    def _create_block(self, n_features, n_transformed_features, **kwargs):\n",
    "        return FeatureTransformerBlock(n_features, n_transformed_features, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-03T22:10:16.943117Z",
     "start_time": "2020-10-03T22:10:16.903093Z"
    }
   },
   "outputs": [],
   "source": [
    "a = torch.randn((n_samples,n_features), dtype=torch.float)\n",
    "trans = FeatureTransformer(n_features, n_transformed_features, 2)\n",
    "test_eq(trans(a).shape, (n_samples, n_transformed_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-03T22:10:17.280004Z",
     "start_time": "2020-10-03T22:10:17.273499Z"
    }
   },
   "outputs": [],
   "source": [
    "#exporti \n",
    "\n",
    "def _split(x):\n",
    "    N = x.shape[-1]//2\n",
    "    return x[:,:N], x[:,N:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-03T22:10:17.498163Z",
     "start_time": "2020-10-03T22:10:17.485867Z"
    }
   },
   "outputs": [],
   "source": [
    "Split = Lambda(_split)\n",
    "res = Split(torch.randn((3,4)))\n",
    "test_eq(len(res), 2)\n",
    "test_eq(res[0].shape, (3,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the `MaskBlock`'s input from the previous part is $\\frac{1}{2}$ of the `n_transformed_features`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-03T22:10:18.417661Z",
     "start_time": "2020-10-03T22:10:18.393357Z"
    }
   },
   "outputs": [],
   "source": [
    "#exporti \n",
    "\n",
    "class TabNetStep(Module):\n",
    "    \n",
    "    @delegates(FeatureTransformer)\n",
    "    def __init__(self, n_features, n_transformed_features, activ=nn.ReLU(), **kwargs):\n",
    "        store_attr('activ')\n",
    "\n",
    "        self.mask = MaskBlock(n_features, n_transformed_features//2)\n",
    "        self.feat_trans = FeatureTransformer(n_features, n_transformed_features, **kwargs)\n",
    "        \n",
    "        \n",
    "    def forward(self, features, prev):\n",
    "        \n",
    "        masked = self.mask(prev) * features \n",
    "        transformed = self.feat_trans(masked)\n",
    "        output, next_input = Split(transformed)\n",
    "        output = self.activ(output)\n",
    "        return output, next_input\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-03T22:10:18.941324Z",
     "start_time": "2020-10-03T22:10:18.921983Z"
    }
   },
   "outputs": [],
   "source": [
    "a = torch.randn((n_samples, n_features))\n",
    "tab_step = TabNetStep(n_features, n_transformed_features)\n",
    "prev = torch.randn((n_samples, n_transformed_features//2))\n",
    "res = tab_step(features=a, prev=prev)\n",
    "test_eq(len(res), 2)\n",
    "test_eq(torch.cat(res, dim=1).shape, (n_samples, n_transformed_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-03T22:10:19.262826Z",
     "start_time": "2020-10-03T22:10:19.233609Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class TabNet(Module):\n",
    "    \n",
    "    @delegates(TabNetStep)\n",
    "    def __init__(self, n_features, n_transformed_features, n_steps=2, **kwargs):        \n",
    "        \n",
    "        self.init = nn.Sequential(\n",
    "            BatchNorm(n_features, ndim=1),\n",
    "            FeatureTransformer(n_features, n_transformed_features), \n",
    "            Split,\n",
    "        )\n",
    "        \n",
    "        self.steps = nn.ModuleList(\n",
    "            [TabNetStep(n_features, n_transformed_features, **kwargs) for i in range(n_steps)]\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def forward(self, features):\n",
    "        outputs = []\n",
    "  \n",
    "        output, next_input = self.init(features)     \n",
    "        \n",
    "        for step in self.steps:        \n",
    "            output, next_input = step(features, next_input)\n",
    "            outputs.append(output)\n",
    "    \n",
    "\n",
    "        outputs = torch.cat(outputs, dim=0)\n",
    "        return torch.sum(outputs, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-03T22:10:19.726572Z",
     "start_time": "2020-10-03T22:10:19.664909Z"
    }
   },
   "outputs": [],
   "source": [
    "a = torch.randn((n_samples, n_features))\n",
    "tabnet = TabNet(n_features, n_transformed_features)\n",
    "test_eq(len(tabnet(a)), n_transformed_features//2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-03T22:14:40.043661Z",
     "start_time": "2020-10-03T22:14:39.705778Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_review_prev_work.ipynb.\n",
      "Converted 01_model.ipynb.\n",
      "Converted Untitled.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
