{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T15:55:56.069111Z",
     "start_time": "2020-10-17T15:55:56.051308Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "#default_exp model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T15:56:05.072875Z",
     "start_time": "2020-10-17T15:55:56.512658Z"
    }
   },
   "outputs": [],
   "source": [
    "#exporti\n",
    "from fastai.tabular.all import * \n",
    "from tabnet.sparsemax import Sparsemax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T14:30:11.255932Z",
     "start_time": "2020-10-17T14:29:59.681335Z"
    }
   },
   "outputs": [],
   "source": [
    "! pip install -e ../../libraries/fastai ../../libraries/fastcore "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T15:56:05.080274Z",
     "start_time": "2020-10-17T15:56:05.074739Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class GBN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "        Ghost Batch Normalization\n",
    "        https://arxiv.org/abs/1705.08741\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, virtual_batch_size=128, momentum=0.02):\n",
    "        super(GBN, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.bn = BatchNorm(self.input_dim, momentum=momentum, ndim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        chunks = x.chunk(int(np.ceil(x.shape[0] / self.virtual_batch_size)), 0)\n",
    "        res = [self.bn(x_) for x_ in chunks]\n",
    "\n",
    "        return torch.cat(res, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T15:56:05.096183Z",
     "start_time": "2020-10-17T15:56:05.082059Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class TabNet(Module):\n",
    "    \n",
    "    def __init__(self, emb_szs, n_cont, out_features, n_d, n_a, n_steps, n_shared_ft_blocks=2,\n",
    "                         n_independent_ft_blocks=2, gamma=1.5, virtual_batch_size=128, momentum=0.2):        \n",
    "        store_attr()\n",
    "        \n",
    "        self.embeds = nn.ModuleList([Embedding(ni, nf) for ni,nf in emb_szs])\n",
    "        self.n_emb = sum(e.embedding_dim for e in self.embeds)\n",
    "        self.n_features = self.n_emb + n_cont\n",
    "        \n",
    "        \n",
    "        intermediate_features = n_d + n_a \n",
    "        \n",
    "        shared_feat_transform = None\n",
    "        if self.n_shared_ft_blocks > 0: \n",
    "#             shared_feat_transform = [[nn.Linear(self.n_features, 2*(n_d + n_a), bias=False), GBN(2*(n_d + n_a), virtual_batch_size=virtual_batch_size,\n",
    "#                       momentum=momentum)]] + \\\n",
    "#                   [[nn.Linear(n_d + n_a, 2*(n_d + n_a), bias=False), GBN(2*(n_d + n_a), virtual_batch_size=virtual_batch_size,\n",
    "#                       momentum=momentum)] for _ in range(self.n_shared_ft_blocks-1)]\n",
    "            shared_feat_transform = [nn.Linear(self.n_features, 2*(n_d + n_a), bias=False)] + \\\n",
    "                  [nn.Linear(n_d + n_a, 2*(n_d + n_a), bias=False) for _ in range(self.n_shared_ft_blocks-1)]\n",
    "\n",
    "            \n",
    "                \n",
    "        self.initial_ft = FeatTransformer(self.n_features, n_d, n_a, shared_feat_transform,\n",
    "                                                n_glu_independent=self.n_independent_ft_blocks,\n",
    "                                                virtual_batch_size=self.virtual_batch_size,\n",
    "                                                momentum=momentum)\n",
    "        \n",
    "        self.initial_bn = BatchNorm(self.n_features, ndim=1)\n",
    "        \n",
    "        \n",
    "        self.att_steps = nn.ModuleList([AttentiveTransformer(n_a, self.n_features,virtual_batch_size, momentum) \n",
    "                                            for i in range(self.n_steps)])\n",
    "        \n",
    "        self.ft_steps = nn.ModuleList([FeatTransformer(self.n_features, n_d, n_a, shared_feat_transform,\n",
    "                                          n_glu_independent=self.n_independent_ft_blocks,\n",
    "                                          virtual_batch_size=self.virtual_batch_size, momentum=momentum)\n",
    "                                           for _ in range(n_steps)])\n",
    "       \n",
    "        self.final_fc = nn.Linear(n_d, out_features)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def forward(self, x_cat, x_cont):\n",
    "        x = self._combine_cat_cont(x_cat, x_cont)\n",
    "        \n",
    "        output = 0\n",
    "        x = self.initial_bn(x)\n",
    "        d, a = self.initial_ft(x)\n",
    "        \n",
    "        prior = torch.ones(self.n_features, device=x_cont.device)\n",
    "        \n",
    "        for i in range(self.n_steps):\n",
    "            M = self.att_steps[i](prior, a)\n",
    "            prior = (self.gamma - M)*prior\n",
    "            res = M * x\n",
    "            d, a = self.ft_steps[i](res)\n",
    "            output = output + nn.functional.relu(d)\n",
    "        \n",
    "        res = self.final_fc(output)\n",
    "        return res\n",
    "    \n",
    "    def _combine_cat_cont(self, x_cat, x_cont):\n",
    "        if self.n_emb != 0:\n",
    "            x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)]\n",
    "            x = torch.cat(x, 1)\n",
    "        if self.n_cont != 0:\n",
    "            x = torch.cat([x, x_cont], 1) if self.n_emb != 0 else x_cont\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T15:56:05.114564Z",
     "start_time": "2020-10-17T15:56:05.097867Z"
    }
   },
   "outputs": [],
   "source": [
    "class FeatTransformer(torch.nn.Module):\n",
    "    def __init__(self, input_dim, n_d, n_a, shared_layers, n_glu_independent,\n",
    "                 virtual_batch_size=128, momentum=0.02):\n",
    "        super(FeatTransformer, self).__init__()\n",
    "        \"\"\"\n",
    "        Initialize a feature transformer.\n",
    "        Parameters\n",
    "        ----------\n",
    "        - input_dim : int\n",
    "            Input size\n",
    "        - output_dim : int\n",
    "            Outpu_size\n",
    "        - n_glu_independant\n",
    "        - shared_blocks : torch.nn.ModuleList\n",
    "            The shared block that should be common to every step\n",
    "        - momentum : float\n",
    "            Float value between 0 and 1 which will be used for momentum in batch norm\n",
    "        \"\"\"\n",
    "\n",
    "        params = {\n",
    "            'n_glu': n_glu_independent,\n",
    "            'virtual_batch_size': virtual_batch_size,\n",
    "            'momentum': momentum\n",
    "        }\n",
    "        self.n_d, self.n_a = n_d, n_a\n",
    "        output_dim = n_d + n_a \n",
    "        if shared_layers is None:\n",
    "            # no shared layers\n",
    "            self.shared = torch.nn.Identity()\n",
    "            is_first = True\n",
    "        else:\n",
    "            self.shared = GLU_Block(input_dim, output_dim,\n",
    "                                    first=True,\n",
    "                                    shared_layers=shared_layers,\n",
    "                                    n_glu=len(shared_layers),\n",
    "                                    virtual_batch_size=virtual_batch_size,\n",
    "                                    momentum=momentum)\n",
    "            is_first = False\n",
    "\n",
    "        if n_glu_independent == 0:\n",
    "            # no independent layers\n",
    "            self.specifics = torch.nn.Identity()\n",
    "        else:\n",
    "            spec_input_dim = input_dim if is_first else output_dim\n",
    "            self.specifics = GLU_Block(spec_input_dim, output_dim,\n",
    "                                       first=is_first,\n",
    "                                       **params)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.shared(x)\n",
    "        res = self.specifics(x)\n",
    "        \n",
    "        d, a = res[:,:self.n_d], res[:,self.n_d:]\n",
    "        return d, a\n",
    "\n",
    "\n",
    "class GLU_Block(torch.nn.Module):\n",
    "    \"\"\"\n",
    "        Independant GLU block, specific to each step\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, n_glu=2, first=False, shared_layers=None,\n",
    "                 virtual_batch_size=128, momentum=0.02):\n",
    "        super(GLU_Block, self).__init__()\n",
    "        self.first = first\n",
    "        self.shared_layers = shared_layers\n",
    "        self.n_glu = n_glu\n",
    "        self.glu_layers = torch.nn.ModuleList()\n",
    "\n",
    "        params = {\n",
    "            'virtual_batch_size': virtual_batch_size,\n",
    "            'momentum': momentum\n",
    "        }\n",
    "\n",
    "        fc = shared_layers[0] if shared_layers else None\n",
    "        self.glu_layers.append(GLU_Layer(input_dim, output_dim,\n",
    "                                         fc=fc,\n",
    "                                         **params))\n",
    "        for glu_id in range(1, self.n_glu):\n",
    "            fc = shared_layers[glu_id] if shared_layers else None\n",
    "            self.glu_layers.append(GLU_Layer(output_dim, output_dim,\n",
    "                                             fc=fc,\n",
    "                                             **params))\n",
    "\n",
    "    def forward(self, x):\n",
    "        scale = torch.sqrt(torch.FloatTensor([0.5]).to(x.device))\n",
    "        if self.first:  # the first layer of the block has no scale multiplication\n",
    "            x = self.glu_layers[0](x)\n",
    "            layers_left = range(1, self.n_glu)\n",
    "        else:\n",
    "            layers_left = range(self.n_glu)\n",
    "\n",
    "        for glu_id in layers_left:\n",
    "            x = torch.add(x, self.glu_layers[glu_id](x))\n",
    "            x = x*scale\n",
    "        return x\n",
    "\n",
    "\n",
    "class GLU_Layer(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, fc=None,\n",
    "                 virtual_batch_size=128, momentum=0.02):\n",
    "        super(GLU_Layer, self).__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        if fc:\n",
    "#             self.fc, self.bn = fc\n",
    "            self.fc = fc\n",
    "        else:\n",
    "#             self.fc = nn.Linear(input_dim, 2*output_dim, bias=False)\n",
    "# #         initialize_glu(self.fc, input_dim, 2*output_dim)\n",
    "#             self.bn = GBN(2*output_dim, virtual_batch_size=virtual_batch_size,\n",
    "#                           momentum=momentum)\n",
    "            self.fc = nn.Linear(input_dim, 2*output_dim, bias=False)\n",
    "    \n",
    "        self.bn = GBN(2*output_dim, virtual_batch_size=virtual_batch_size, momentum=momentum)\n",
    "\n",
    "        self.glu = nn.GLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.bn(x)\n",
    "        out = torch.mul(x[:, :self.output_dim], torch.sigmoid(x[:, self.output_dim:]))\n",
    "#         out = self.glu(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T15:56:05.121095Z",
     "start_time": "2020-10-17T15:56:05.116029Z"
    }
   },
   "outputs": [],
   "source": [
    "class AttentiveTransformer(Module):\n",
    "    \n",
    "    def __init__(self, n_a, in_features, virtual_batch_size, momentum):\n",
    "        store_attr()\n",
    "        self.fc = nn.Linear(n_a, in_features)\n",
    "        self.bn = GBN(in_features, virtual_batch_size, momentum)\n",
    "        self.sparsemax = Sparsemax()\n",
    "        \n",
    "    def forward(self, prior, a):\n",
    "        a = self.fc(a)\n",
    "        a = self.bn(a)\n",
    "        a = prior * a\n",
    "        M = self.sparsemax(a)\n",
    "        return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T15:56:05.128139Z",
     "start_time": "2020-10-17T15:56:05.122478Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class FeatureTransformer(Module):\n",
    "    \n",
    "    def __init__(self, shared_blocks, n_d, n_a, n_independent_ft_blocks, virtual_batch_size, norm=math.sqrt(0.5)):\n",
    "        store_attr()\n",
    "        intermediate_features = n_d + n_a\n",
    "        steps = [FeatureTransformerBlock(intermediate_features, intermediate_features, virtual_batch_size) \n",
    "                             for _ in range(n_independent_ft_blocks)]\n",
    "        self.steps = nn.ModuleList([*shared_blocks, *steps])\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        first_step = self.steps[0]\n",
    "        x = first_step(x)\n",
    "        for step in self.steps[1:]:\n",
    "            x = (step(x) + x)*self.norm\n",
    "    \n",
    "        d, a = x[:,:self.n_d], x[:,self.n_d:]\n",
    "        return d, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T15:56:05.134167Z",
     "start_time": "2020-10-17T15:56:05.129473Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class FeatureTransformerBlock(Module):\n",
    "    def __init__(self, in_features, intermediate_features, virtual_batch_size):\n",
    "        store_attr()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_features, 2*intermediate_features),\n",
    "            GBN(2*intermediate_features),\n",
    "            nn.GLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T15:56:05.444125Z",
     "start_time": "2020-10-17T15:56:05.136173Z"
    }
   },
   "outputs": [],
   "source": [
    "N = 3\n",
    "n_features = 32\n",
    "n_d = n_a = 7 \n",
    "n_steps = 3\n",
    "out_features = 10\n",
    "virtual_batch_size = 5\n",
    "\n",
    "a = torch.randn((N, n_features))\n",
    "ft = FeatureTransformerBlock(n_features, n_d+n_a, virtual_batch_size)\n",
    "test_eq(ft(a).shape, (N, n_d+n_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T15:56:05.699809Z",
     "start_time": "2020-10-17T15:56:05.445742Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a = torch.randn((N, n_features))\n",
    "tabnet = TabNet([], n_features, out_features, n_d, n_a, n_steps, virtual_batch_size=virtual_batch_size)\n",
    "test_eq(tabnet(a, a).shape, (N, out_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adult "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T15:56:06.081223Z",
     "start_time": "2020-10-17T15:56:05.701361Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49</td>\n",
       "      <td>Private</td>\n",
       "      <td>101320</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>1902</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;=50k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>236746</td>\n",
       "      <td>Masters</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>10520</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;=50k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>96185</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;50k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38</td>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>112847</td>\n",
       "      <td>Prof-school</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Asian-Pac-Islander</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;=50k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>82297</td>\n",
       "      <td>7th-8th</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;50k</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass  fnlwgt     education  education-num  \\\n",
       "0   49            Private  101320    Assoc-acdm           12.0   \n",
       "1   44            Private  236746       Masters           14.0   \n",
       "2   38            Private   96185       HS-grad            NaN   \n",
       "3   38       Self-emp-inc  112847   Prof-school           15.0   \n",
       "4   42   Self-emp-not-inc   82297       7th-8th            NaN   \n",
       "\n",
       "        marital-status        occupation    relationship                 race  \\\n",
       "0   Married-civ-spouse               NaN            Wife                White   \n",
       "1             Divorced   Exec-managerial   Not-in-family                White   \n",
       "2             Divorced               NaN       Unmarried                Black   \n",
       "3   Married-civ-spouse    Prof-specialty         Husband   Asian-Pac-Islander   \n",
       "4   Married-civ-spouse     Other-service            Wife                Black   \n",
       "\n",
       "       sex  capital-gain  capital-loss  hours-per-week  native-country salary  \n",
       "0   Female             0          1902              40   United-States  >=50k  \n",
       "1     Male         10520             0              45   United-States  >=50k  \n",
       "2   Female             0             0              32   United-States   <50k  \n",
       "3     Male             0             0              40   United-States  >=50k  \n",
       "4   Female             0             0              50   United-States   <50k  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult_path = untar_data(URLs.ADULT_SAMPLE)\n",
    "df = pd.read_csv(adult_path/'adult.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T15:56:06.285358Z",
     "start_time": "2020-10-17T15:56:06.082629Z"
    }
   },
   "outputs": [],
   "source": [
    "splits = RandomSplitter(valid_pct=0.2)(range_of(df))\n",
    "to = TabularPandas(df, procs=[Categorify, FillMissing,Normalize],\n",
    "                   cat_names = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race'],\n",
    "                   cont_names = ['age', 'fnlwgt', 'education-num'],\n",
    "                   y_names='salary',\n",
    "                   splits=splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T15:56:29.942616Z",
     "start_time": "2020-10-17T15:56:06.286878Z"
    }
   },
   "outputs": [],
   "source": [
    "dls = to.dataloaders(bs=1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T15:56:30.017248Z",
     "start_time": "2020-10-17T15:56:29.944095Z"
    }
   },
   "outputs": [],
   "source": [
    "model = TabNet(get_emb_sz(to), len(to.cont_names), dls.c, n_d=16, n_a=16, \n",
    "                    n_steps=5, virtual_batch_size=256, gamma=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T15:56:30.022647Z",
     "start_time": "2020-10-17T15:56:30.018661Z"
    }
   },
   "outputs": [],
   "source": [
    "opt_func = partial(Adam, eps=1e-5)\n",
    "learn = Learner(dls, model, CrossEntropyLossFlat(), opt_func=opt_func, lr=3e-2, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T15:56:37.942097Z",
     "start_time": "2020-10-17T15:56:30.024173Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(lr_min=0.03019951581954956, lr_steep=0.05754399299621582)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T16:02:04.315488Z",
     "start_time": "2020-10-17T15:57:18.116719Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.517746</td>\n",
       "      <td>0.477690</td>\n",
       "      <td>0.772113</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.485005</td>\n",
       "      <td>0.460263</td>\n",
       "      <td>0.767353</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.461149</td>\n",
       "      <td>0.425104</td>\n",
       "      <td>0.798065</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.445528</td>\n",
       "      <td>0.419973</td>\n",
       "      <td>0.794994</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.432625</td>\n",
       "      <td>0.416918</td>\n",
       "      <td>0.791155</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.426365</td>\n",
       "      <td>0.412487</td>\n",
       "      <td>0.798986</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.418936</td>\n",
       "      <td>0.403552</td>\n",
       "      <td>0.810964</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.414781</td>\n",
       "      <td>0.402001</td>\n",
       "      <td>0.812807</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.411042</td>\n",
       "      <td>0.411354</td>\n",
       "      <td>0.803747</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.407411</td>\n",
       "      <td>0.403234</td>\n",
       "      <td>0.813268</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.405911</td>\n",
       "      <td>0.409951</td>\n",
       "      <td>0.811425</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.403800</td>\n",
       "      <td>0.397260</td>\n",
       "      <td>0.814189</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.398584</td>\n",
       "      <td>0.387643</td>\n",
       "      <td>0.823710</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.392442</td>\n",
       "      <td>0.378914</td>\n",
       "      <td>0.823249</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.388986</td>\n",
       "      <td>0.389503</td>\n",
       "      <td>0.814343</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.389548</td>\n",
       "      <td>0.388710</td>\n",
       "      <td>0.818950</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.386558</td>\n",
       "      <td>0.382885</td>\n",
       "      <td>0.819257</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.384947</td>\n",
       "      <td>0.388655</td>\n",
       "      <td>0.816493</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.384237</td>\n",
       "      <td>0.397667</td>\n",
       "      <td>0.811271</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.384535</td>\n",
       "      <td>0.378867</td>\n",
       "      <td>0.820178</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.385290</td>\n",
       "      <td>0.385207</td>\n",
       "      <td>0.812961</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.382117</td>\n",
       "      <td>0.381108</td>\n",
       "      <td>0.816646</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.378954</td>\n",
       "      <td>0.379461</td>\n",
       "      <td>0.818796</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.378188</td>\n",
       "      <td>0.375627</td>\n",
       "      <td>0.820946</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.376437</td>\n",
       "      <td>0.376866</td>\n",
       "      <td>0.821560</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.375250</td>\n",
       "      <td>0.374760</td>\n",
       "      <td>0.824785</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.372953</td>\n",
       "      <td>0.369782</td>\n",
       "      <td>0.825399</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.371374</td>\n",
       "      <td>0.373648</td>\n",
       "      <td>0.821407</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.371403</td>\n",
       "      <td>0.380738</td>\n",
       "      <td>0.810964</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.371876</td>\n",
       "      <td>0.373031</td>\n",
       "      <td>0.822174</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.369775</td>\n",
       "      <td>0.371998</td>\n",
       "      <td>0.819257</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.370066</td>\n",
       "      <td>0.374156</td>\n",
       "      <td>0.819717</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.369569</td>\n",
       "      <td>0.366867</td>\n",
       "      <td>0.822328</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.367666</td>\n",
       "      <td>0.366955</td>\n",
       "      <td>0.822021</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.365953</td>\n",
       "      <td>0.363327</td>\n",
       "      <td>0.826014</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.364872</td>\n",
       "      <td>0.374004</td>\n",
       "      <td>0.822174</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.364590</td>\n",
       "      <td>0.368525</td>\n",
       "      <td>0.827242</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.363396</td>\n",
       "      <td>0.361450</td>\n",
       "      <td>0.824017</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.361826</td>\n",
       "      <td>0.366473</td>\n",
       "      <td>0.825553</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.361620</td>\n",
       "      <td>0.363119</td>\n",
       "      <td>0.826474</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.361690</td>\n",
       "      <td>0.367130</td>\n",
       "      <td>0.824785</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.365912</td>\n",
       "      <td>0.382339</td>\n",
       "      <td>0.824017</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.372320</td>\n",
       "      <td>0.372950</td>\n",
       "      <td>0.822942</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.372975</td>\n",
       "      <td>0.367830</td>\n",
       "      <td>0.826014</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.370457</td>\n",
       "      <td>0.364966</td>\n",
       "      <td>0.830313</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.369662</td>\n",
       "      <td>0.371711</td>\n",
       "      <td>0.822328</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.369570</td>\n",
       "      <td>0.373738</td>\n",
       "      <td>0.822021</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.369298</td>\n",
       "      <td>0.368224</td>\n",
       "      <td>0.822174</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.368098</td>\n",
       "      <td>0.361958</td>\n",
       "      <td>0.828163</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.365312</td>\n",
       "      <td>0.362033</td>\n",
       "      <td>0.830927</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.364153</td>\n",
       "      <td>0.362275</td>\n",
       "      <td>0.823864</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.363243</td>\n",
       "      <td>0.357254</td>\n",
       "      <td>0.833077</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.361304</td>\n",
       "      <td>0.356616</td>\n",
       "      <td>0.834613</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.364039</td>\n",
       "      <td>0.375179</td>\n",
       "      <td>0.818796</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.363169</td>\n",
       "      <td>0.360757</td>\n",
       "      <td>0.832463</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.361448</td>\n",
       "      <td>0.361354</td>\n",
       "      <td>0.833077</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.361329</td>\n",
       "      <td>0.363241</td>\n",
       "      <td>0.827703</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.360903</td>\n",
       "      <td>0.363177</td>\n",
       "      <td>0.828471</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.359941</td>\n",
       "      <td>0.360447</td>\n",
       "      <td>0.829853</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.359376</td>\n",
       "      <td>0.360173</td>\n",
       "      <td>0.831542</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.358895</td>\n",
       "      <td>0.359297</td>\n",
       "      <td>0.832156</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.357709</td>\n",
       "      <td>0.356249</td>\n",
       "      <td>0.832156</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.356380</td>\n",
       "      <td>0.354466</td>\n",
       "      <td>0.834613</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.355977</td>\n",
       "      <td>0.356108</td>\n",
       "      <td>0.831695</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.356046</td>\n",
       "      <td>0.357799</td>\n",
       "      <td>0.831081</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.355984</td>\n",
       "      <td>0.357390</td>\n",
       "      <td>0.833385</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.355249</td>\n",
       "      <td>0.358879</td>\n",
       "      <td>0.829699</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.355185</td>\n",
       "      <td>0.355265</td>\n",
       "      <td>0.833999</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.354526</td>\n",
       "      <td>0.355377</td>\n",
       "      <td>0.835688</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.354312</td>\n",
       "      <td>0.360044</td>\n",
       "      <td>0.834459</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.354792</td>\n",
       "      <td>0.356323</td>\n",
       "      <td>0.829699</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.353930</td>\n",
       "      <td>0.356680</td>\n",
       "      <td>0.834920</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.352530</td>\n",
       "      <td>0.355089</td>\n",
       "      <td>0.835534</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.351799</td>\n",
       "      <td>0.354010</td>\n",
       "      <td>0.834152</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.350810</td>\n",
       "      <td>0.352496</td>\n",
       "      <td>0.837377</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.350121</td>\n",
       "      <td>0.355305</td>\n",
       "      <td>0.833999</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.350437</td>\n",
       "      <td>0.353945</td>\n",
       "      <td>0.835995</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.349636</td>\n",
       "      <td>0.352427</td>\n",
       "      <td>0.834152</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.349462</td>\n",
       "      <td>0.355452</td>\n",
       "      <td>0.834152</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.349239</td>\n",
       "      <td>0.353840</td>\n",
       "      <td>0.839988</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.349115</td>\n",
       "      <td>0.352233</td>\n",
       "      <td>0.836763</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.348139</td>\n",
       "      <td>0.354770</td>\n",
       "      <td>0.834306</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.348139</td>\n",
       "      <td>0.357907</td>\n",
       "      <td>0.832617</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.348343</td>\n",
       "      <td>0.361324</td>\n",
       "      <td>0.831235</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.348338</td>\n",
       "      <td>0.354804</td>\n",
       "      <td>0.835074</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.347413</td>\n",
       "      <td>0.355785</td>\n",
       "      <td>0.836609</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.346891</td>\n",
       "      <td>0.357316</td>\n",
       "      <td>0.833231</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.347497</td>\n",
       "      <td>0.356134</td>\n",
       "      <td>0.835995</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.349413</td>\n",
       "      <td>0.360585</td>\n",
       "      <td>0.829853</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.350273</td>\n",
       "      <td>0.361223</td>\n",
       "      <td>0.827856</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.348293</td>\n",
       "      <td>0.354148</td>\n",
       "      <td>0.834152</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.346830</td>\n",
       "      <td>0.353733</td>\n",
       "      <td>0.837684</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.346055</td>\n",
       "      <td>0.359225</td>\n",
       "      <td>0.835381</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.344060</td>\n",
       "      <td>0.355287</td>\n",
       "      <td>0.834613</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.341997</td>\n",
       "      <td>0.355664</td>\n",
       "      <td>0.835381</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.340502</td>\n",
       "      <td>0.360288</td>\n",
       "      <td>0.830467</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.340497</td>\n",
       "      <td>0.354600</td>\n",
       "      <td>0.835842</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.339194</td>\n",
       "      <td>0.359282</td>\n",
       "      <td>0.834920</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.338513</td>\n",
       "      <td>0.356410</td>\n",
       "      <td>0.833999</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.336640</td>\n",
       "      <td>0.356209</td>\n",
       "      <td>0.835842</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.335958</td>\n",
       "      <td>0.357124</td>\n",
       "      <td>0.836456</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.334370</td>\n",
       "      <td>0.358119</td>\n",
       "      <td>0.835381</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.333727</td>\n",
       "      <td>0.360696</td>\n",
       "      <td>0.833999</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.333704</td>\n",
       "      <td>0.359229</td>\n",
       "      <td>0.835995</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.333248</td>\n",
       "      <td>0.360562</td>\n",
       "      <td>0.835074</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.332789</td>\n",
       "      <td>0.360980</td>\n",
       "      <td>0.832770</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.331795</td>\n",
       "      <td>0.363037</td>\n",
       "      <td>0.830467</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.331138</td>\n",
       "      <td>0.361284</td>\n",
       "      <td>0.836916</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.330112</td>\n",
       "      <td>0.364239</td>\n",
       "      <td>0.835074</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.329081</td>\n",
       "      <td>0.362175</td>\n",
       "      <td>0.832924</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.327728</td>\n",
       "      <td>0.362708</td>\n",
       "      <td>0.830927</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.327902</td>\n",
       "      <td>0.364543</td>\n",
       "      <td>0.833231</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.327840</td>\n",
       "      <td>0.374080</td>\n",
       "      <td>0.830927</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.328557</td>\n",
       "      <td>0.365600</td>\n",
       "      <td>0.834767</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.327230</td>\n",
       "      <td>0.364071</td>\n",
       "      <td>0.833538</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.326461</td>\n",
       "      <td>0.366084</td>\n",
       "      <td>0.832002</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.324727</td>\n",
       "      <td>0.366687</td>\n",
       "      <td>0.831542</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.323743</td>\n",
       "      <td>0.367983</td>\n",
       "      <td>0.830160</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.322576</td>\n",
       "      <td>0.367154</td>\n",
       "      <td>0.831081</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.321156</td>\n",
       "      <td>0.370216</td>\n",
       "      <td>0.830160</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.320145</td>\n",
       "      <td>0.368409</td>\n",
       "      <td>0.833077</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.319308</td>\n",
       "      <td>0.368637</td>\n",
       "      <td>0.831542</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.318597</td>\n",
       "      <td>0.369338</td>\n",
       "      <td>0.829238</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.317873</td>\n",
       "      <td>0.371125</td>\n",
       "      <td>0.831081</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.316900</td>\n",
       "      <td>0.369948</td>\n",
       "      <td>0.827856</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.316375</td>\n",
       "      <td>0.370025</td>\n",
       "      <td>0.828778</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.315679</td>\n",
       "      <td>0.370102</td>\n",
       "      <td>0.831081</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.314487</td>\n",
       "      <td>0.371036</td>\n",
       "      <td>0.830313</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.314541</td>\n",
       "      <td>0.371901</td>\n",
       "      <td>0.829085</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.313656</td>\n",
       "      <td>0.371913</td>\n",
       "      <td>0.830313</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.312837</td>\n",
       "      <td>0.372883</td>\n",
       "      <td>0.830927</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.311834</td>\n",
       "      <td>0.373782</td>\n",
       "      <td>0.830467</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.310877</td>\n",
       "      <td>0.374655</td>\n",
       "      <td>0.827856</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.310518</td>\n",
       "      <td>0.374437</td>\n",
       "      <td>0.830160</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.310234</td>\n",
       "      <td>0.374190</td>\n",
       "      <td>0.829238</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.309169</td>\n",
       "      <td>0.374514</td>\n",
       "      <td>0.829392</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.309029</td>\n",
       "      <td>0.373637</td>\n",
       "      <td>0.831849</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.309001</td>\n",
       "      <td>0.374236</td>\n",
       "      <td>0.826935</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.308584</td>\n",
       "      <td>0.375795</td>\n",
       "      <td>0.828317</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.307669</td>\n",
       "      <td>0.374526</td>\n",
       "      <td>0.828778</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.307640</td>\n",
       "      <td>0.375822</td>\n",
       "      <td>0.829853</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.307873</td>\n",
       "      <td>0.375508</td>\n",
       "      <td>0.828778</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.307616</td>\n",
       "      <td>0.375252</td>\n",
       "      <td>0.831235</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.307277</td>\n",
       "      <td>0.377025</td>\n",
       "      <td>0.832002</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.307044</td>\n",
       "      <td>0.375551</td>\n",
       "      <td>0.829392</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.306910</td>\n",
       "      <td>0.376507</td>\n",
       "      <td>0.829699</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.307034</td>\n",
       "      <td>0.377232</td>\n",
       "      <td>0.829085</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.306628</td>\n",
       "      <td>0.376978</td>\n",
       "      <td>0.829699</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.306645</td>\n",
       "      <td>0.376735</td>\n",
       "      <td>0.828931</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.306659</td>\n",
       "      <td>0.376191</td>\n",
       "      <td>0.830467</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(150, 1e-1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Forest Cover DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T15:37:10.674431Z",
     "start_time": "2020-10-17T15:37:10.671613Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_dir = Path('./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T13:48:40.668498Z",
     "start_time": "2020-10-16T13:48:40.664283Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def extract_gzip(file, dest=None):\n",
    "    import gzip\n",
    "    dest = dest or Path(dest)\n",
    "    with gzip.open(file, 'rb') as f_in:\n",
    "        with open(dest / file.stem, 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T13:48:41.010547Z",
     "start_time": "2020-10-16T13:48:41.004908Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "forest_type_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz'\n",
    "forest_path = untar_data(forest_type_url, dest=data_dir, extract_func=extract_gzip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T13:48:41.186241Z",
     "start_time": "2020-10-16T13:48:41.180661Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "target = \"Covertype\"\n",
    "\n",
    "cat_names = [\n",
    "    \"Wilderness_Area1\", \"Wilderness_Area2\", \"Wilderness_Area3\",\n",
    "    \"Wilderness_Area4\", \"Soil_Type1\", \"Soil_Type2\", \"Soil_Type3\", \"Soil_Type4\",\n",
    "    \"Soil_Type5\", \"Soil_Type6\", \"Soil_Type7\", \"Soil_Type8\", \"Soil_Type9\",\n",
    "    \"Soil_Type10\", \"Soil_Type11\", \"Soil_Type12\", \"Soil_Type13\", \"Soil_Type14\",\n",
    "    \"Soil_Type15\", \"Soil_Type16\", \"Soil_Type17\", \"Soil_Type18\", \"Soil_Type19\",\n",
    "    \"Soil_Type20\", \"Soil_Type21\", \"Soil_Type22\", \"Soil_Type23\", \"Soil_Type24\",\n",
    "    \"Soil_Type25\", \"Soil_Type26\", \"Soil_Type27\", \"Soil_Type28\", \"Soil_Type29\",\n",
    "    \"Soil_Type30\", \"Soil_Type31\", \"Soil_Type32\", \"Soil_Type33\", \"Soil_Type34\",\n",
    "    \"Soil_Type35\", \"Soil_Type36\", \"Soil_Type37\", \"Soil_Type38\", \"Soil_Type39\",\n",
    "    \"Soil_Type40\"\n",
    "]\n",
    "\n",
    "cont_names = [\n",
    "    \"Elevation\", \"Aspect\", \"Slope\", \"Horizontal_Distance_To_Hydrology\",\n",
    "    \"Vertical_Distance_To_Hydrology\", \"Horizontal_Distance_To_Roadways\",\n",
    "    \"Hillshade_9am\", \"Hillshade_Noon\", \"Hillshade_3pm\",\n",
    "    \"Horizontal_Distance_To_Fire_Points\"\n",
    "]\n",
    "\n",
    "feature_columns = (\n",
    "    cont_names + cat_names + [target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T13:48:42.546539Z",
     "start_time": "2020-10-16T13:48:41.339728Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(forest_path, header=None, names=feature_columns); df.head()\n",
    "df = df_shrink(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T13:48:44.304191Z",
     "start_time": "2020-10-16T13:48:42.548321Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "procs = [Categorify, FillMissing, Normalize]\n",
    "splits = RandomSplitter(0.05)(range_of(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T13:48:46.338677Z",
     "start_time": "2020-10-16T13:48:44.306260Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "to = TabularPandas(df, procs, cat_names, cont_names, y_names=target, y_block = CategoryBlock(), splits=splits)\n",
    "dls = to.dataloaders(bs=64*64*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T14:24:55.037584Z",
     "start_time": "2020-10-16T14:24:54.987223Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = TabNet(get_emb_sz(to), len(to.cont_names), dls.c, n_d=64, n_a=64, n_steps=5, virtual_batch_size=256)\n",
    "opt_func = partial(Adam, wd=0.01, eps=1e-5)\n",
    "learn = Learner(dls, model, CrossEntropyLossFlat(), opt_func=opt_func, lr=3e-2, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T15:10:19.659951Z",
     "start_time": "2020-10-16T14:25:00.453843Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Poker Hand DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T16:30:58.644206Z",
     "start_time": "2020-10-16T16:30:58.641154Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "BASE_DIR = Path.home().joinpath('data/tabnet/poker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T16:30:58.868973Z",
     "start_time": "2020-10-16T16:30:58.835197Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(BASE_DIR.joinpath('train.csv'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T15:11:23.692614Z",
     "start_time": "2020-10-16T15:11:23.689302Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cat_names = ['S1', 'S2', 'S3', 'S4', 'S5', 'C1', 'C2', 'C3', 'C4', 'C5']\n",
    "cont_names = []\n",
    "target = ['hand']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T15:11:28.174284Z",
     "start_time": "2020-10-16T15:11:28.067758Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "procs = [Categorify, Normalize]\n",
    "splits = RandomSplitter(0.05)(range_of(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T15:11:32.914397Z",
     "start_time": "2020-10-16T15:11:32.856131Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "to = TabularPandas(df, procs, cat_names, cont_names, y_names=target, y_block = CategoryBlock(), splits=splits)\n",
    "dls = to.dataloaders(bs=64*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T09:22:56.925831Z",
     "start_time": "2020-10-17T09:22:56.914990Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = TabNet(get_emb_sz(to), len(to.cont_names), dls.c, n_d=16, n_a=16, \n",
    "                    n_steps=5, virtual_batch_size=256, gamma=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T15:11:48.874409Z",
     "start_time": "2020-10-16T15:11:48.869793Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "opt_func = partial(Adam, eps=1e-5)\n",
    "learn = Learner(dls, model, CrossEntropyLossFlat(), opt_func=opt_func, lr=3e-2, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T15:14:57.291027Z",
     "start_time": "2020-10-16T15:12:00.643932Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T11:18:30.313110Z",
     "start_time": "2020-10-17T11:18:29.982291Z"
    }
   },
   "outputs": [],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "304.475px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
